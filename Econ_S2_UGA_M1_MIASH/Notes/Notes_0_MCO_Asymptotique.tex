\documentclass[12pt, reqno]{amsart}
%\documentclass[12pt, reqno, fleqn]{amsart}
\input{header_fr}
%\raggedright \onehalfspacing \setlength{\parindent}{20pt} \allowdisplaybreaks
%\pagestyle{fancy} \lhead{\'Econométrie 1} \rhead{L3 MIASH, 2016-2017}
%\pagestyle{fancy} \rfoot{\textcopyright \ \  Michal W. Urdanivia}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{\'Econométrie 2}
\fancyhead[R]{Master MIASH , 2016-2017}
\fancyfoot[R]{\textcopyright \ \  Michal W. Urdanivia}
\usepackage{authblk}
%\usepackage{sectsty}
\usepackage{lipsum}
\title{Propriétés asymptotiques de  l' estimateur des moindres carrés}
\date{Année universitaire 2016-2017}
\begin{document}
\maketitle
\section{Introduction}
Dans ces notes nous allons nous intéresser aux propriétés asymptotiques de l'estimateur des moindres carrés ordinaires(MCO par la suite) pour le modèle de régression linéaire, lequel a été longuement étudié dans le cours d'économétrie du semestre 1.
Rappelons qu'il s'agit de l'estimateur du vecteur $K\times 1$, $\beta$, dans le modèle où la relation entre la variable dépendante $Y$ et le vecteur $K\times 1$ de régresseurs $X$ est linéaire par rapport à $\beta$,

\begin{align*}
Y &=X^\top\beta +U
\end{align*}

où $U$ est le terme d'erreur du modèle. L'estimateur des MCO peut alors s'écrire,

\begin{align*}
\widehat{\beta}_n &= \left(\sumobs X_iX_i^\top\right)^{-1}\sumobs X_iY_i
\end{align*}

pour un échantillon $\{(Y_i, X_i), i = 1,...,n\}$ de $Y$ et de $X$. Observez que nous indiçons l'estimateur par rapport à la taille de l'échantillon car nous allons étudier les propriétés de $\widehat{\beta}_n $ pour $n$ "devenant de plus en plus grand". Autrement dit les propriétés asymptotiques de $\widehat{\beta}_n$.

\section{Le modèle}
On s'intéresse à la relation entre une variable $Y\in \mathbb{R}$, appelée \emph{variable dépendante}, et un vecteur $X\in \mathbb{R}^K$, de variables appelées  régresseurs. Pour cela nous disposons de données $\{(Y_i, X_i)\}_{i=1}^n$, et le modèle que nous considérons est un modèle de régression linéaire défini par les hypothèses suivantes.
\begin{assumption}
Les données  $\{(Y_i, X_i), i = 1,...,n\}$ sont un échantillon i.i.d.
\label{hy1}
\end{assumption}
\begin{assumption} $Y_i$  et $X_i$ vérifient,
\begin{align*} 
Y_i= X_i^\top\beta + U_i \ \ i = 1,...,n
\end{align*}
où $U_i$ est une variable inobservée(ou terme d'erreur) vérifiant $\Er(U_i) = 0$.
\label{hy2} 
\end{assumption}
\begin{assumption}$X_i$ est (faiblement)exogène par rapport à $U_i$,
\begin{align*}
\Er(X_iU_i) = 0
\end{align*}
\label{hy3} 
\end{assumption}
\begin{assumption} 
La matrice $\Er(X_iX_i^\top)$ est finie et définie positive.
\label{hy4}
\end{assumption}
\begin{assumption}
$\Er(X_{i,k}^4) < \infty$ , pour tout $k=1,...,K$.
\label{hy5}
\end{assumption}
\begin{assumption}
$\Er(U_i^4) < \infty$
\label{hy6}
\end{assumption}
\begin{assumption}
$\Er(U_i^2X_iX_i^\top)$ est définie positive.
\label{hy7}
\end{assumption}

\section{Convergence}

L'estimateur des MCO est convergent pour $\beta$ si $\widehat{\beta}_n \limp \beta$, ce qui est établi par le théorème suivant.
\begin{theoreme}(\textbf{Convergence de l'estimateur des moindres carrés})
Sous les hypothèses \ref{hy1} - \ref{hy4}, $\widehat{\beta}_n \limp \beta$.
\end{theoreme}
\begin{proof}
$\widehat{\beta}_n$ peut s'écrire,
\begin{align}
\widehat{\beta}_n = \beta +  \left(n^{-1}\sumobs X_iX_i^\top\right)^{-1}n^{-1}\sumobs X_iU_i
\label{eq1}
\end{align}
Les termes $U_i$'s et les termes $X_iU_i$'s sont i.i.d. sous l'hypothèse \ref{hy1}. Dans ce cas, par la loi faible de grands nombres,
\begin{align*}
n^{-1} \sumobs X_iU_i \limp \Er(X_iU_i) = 0
\end{align*}
Où l'on utilise \ref{hy3}. Dans la mesure où $E(X_iX_i^\top)$ est finie sous l'hypothèse \ref{hy4} nous avons par la loi faible des grand nombres,
\begin{align*}
n^{-1} \sumobs X_iX_i^\top \limp \Er(X_iX_i^\top)
\end{align*}
et comme  $E(X_iX_i^\top)$ est définie positive, nous avons  par le théorème de Slutsky,
\begin{align}
\left(n^{-1} \sumobs X_iX_i^\top\right)^{-1} \limp \left(\Er(X_iX_i^\top)\right)^{-1}
\label{eq2}
\end{align}
Et par conséquent,
\begin{align*}
\left(n^{-1}\sumobs X_iX_i^\top\right)^{-1}n^{-1}\sumobs X_iU_i \limp 0
\end{align*}
et donc,
\begin{align*}
\widehat{\beta}_n \limp \beta
\end{align*}
\end{proof}

\section{Distribution asymptotique}
Le résultat suivant établit la distribution asymptotique de l'estimateur des moindres carrés.
\begin{theoreme}\label{th2}(\textbf{Normalité asymptotique})
Sous les hypothèses \ref{hy1}-\ref{hy7},
\begin{align*}
n^{1/2}(\widehat{\beta}_n - \beta) \limd \mathcal{N}(0, V)
\end{align*}
où
\begin{align*}
V = Q^{-1} \Omega Q^{-1}, \ \ Q = \Er(X_iX_i^\top), \ \ \Omega =  \Er(U_i^2X_iX_i^\top)
\end{align*}
\end{theoreme}
\begin{proof}
Nous avons en utilisant \eqref{eq1},
\begin{align*}
n^{1/2}(\widehat{\beta}_n - \beta) = \left(n^{-1}\sumobs X_iX_i^\top\right)^{-1}n^{-1/2}\sumobs X_iU_i 
\end{align*}
En raisons de l'hypothèse \ref{hy3}, $\Er(X_iU_i) = 0$. En outre par l'inégalité de Cauchy-Schwartz et sous les hypothèses \ref{hy5} , \ref{hy6},  et \ref{hy7}, nous avons pour l'élément  $(j, k)$, $j,k = 1,...,K$, de $\Var(X_iU_i) = \Er(U_i^2X_iX_i^\top)$, soit $\Er(U_i^2X_{i,j}X_{i,k})$,
\begin{align*}
\Er\left(\abs{U_i^2 X_{i,j}  X_{i,k}}\right) \leq \left[\Er(U_i^4)\Er(X_{i,j}^2X_{i,k}^2)\right]^{1/2} \leq  \left[\Er(U_i^4)^{1/2}
\Er(X_ {i,j}^4\Er(X_{i,k}^4))\right]^{1/4} < \infty 
\end{align*}
Par le théorème central-limite,
\begin{align}
n^{-1/2}\sumobs X_iU_i  \limd \mathcal{N}\left(0, \Er(U_i^2X_iX_i^\top)\right) = \mathcal{N}(0, \Omega)
\label{eq3}
\end{align}
Finalement \eqref{eq2}, \eqref{eq3} et le théorème de convergence de Cramer(son extension multivariée) impliquent que,
\begin{align*}
\left(n^{-1}\sumobs X_iX_i^\top\right)^{-1}n^{-1/2}\sumobs X_iU_i \limd Q^{-1} \mathcal{N}(0, \Omega) = \mathcal{N}(0, Q^{-1}\Omega Q^{-1})
\end{align*}
\end{proof}
\begin{remarque}\label{re1}
Les hypothèses du théorème \ref{th2} n'excluent pas le cas où la variance conditionnelle des $U_i$'s est une fonction de $X_i$, i.e. il est possible que les termes d'erreur $U_i$'s soient \emph{hétéroscédastiques}: $\Er(U_i^2|X_i) = \sigma^2(X_i)$ pour une fonction $\sigma^2: \mathbb{R}^K \mapsto \mathbb{R}$.
\end{remarque}
\begin{remarque}\label{re2}
La matrice de variances-covariances asymptotique de $\widehat{\beta}_n$ est donnée par la formule "en sandwich",
\begin{align*}
V = \left(\Er(X_iX_i^\top)\right)^{-1} \Er(U_i^2X_iX_i^\top) \left(\Er(X_iX_i^\top)\right)^{-1}
\end{align*}
Si nous imposons la condition que $\Er(U_i^2|X_i) = \sigma^2$, alors $V$ se simplifie en la matrice des variances-covariances \emph{homoscédastique},
\begin{align}
V = \sigma^2\left(\Er(X_iX_i^\top)\right)^{-1}
\label{eq4}
\end{align}
En effet par la règle des conditionnements successifs,
\begin{align*}
\Er(U_i^2X_iX_i^\top) = \Er\left(\Er(U_i^2X_iX_i^\top) | X_i) \right) = \Er\left(X_iX_i^\top\Er(U_i^2|X_i)\right) = \sigma^2\Er(X_iX_i^\top)
\end{align*}
ainsi dans ce cas, 
\begin{align*}
\Omega = \sigma^2 Q \ \ \textrm{et,} \ \ V = Q^{-1}\Omega  Q^{-1} = \sigma^2  Q^{-1} =\sigma^2\left(\Er(X_iX_i^\top)\right)^{-1}
\end{align*}
\end{remarque}

\section{Estimation de la matrice des variances-covariances}
A partir d'un estimateur de $\beta$, nous pouvons construire les résidus $\widehat{U}_i = Y_i - X_i^\top \widehat{\beta}_n$.  Considérons l'estimateur suivant de $V$ obtenu par application du principe d'analogie,
\begin{align*}
\widehat{V}_n = \widehat{Q}_n^{-1} \widehat{\Omega}_n \widehat{Q}_n^{-1}
\end{align*}
où,
\begin{align*}
\widehat{Q}_n = n^{-1}\sumobs X_iX_i^\top \ \ , \ \ \widehat{\Omega}_n = n^{-1}\sumobs \widehat{U}_i^2 X_iX_i^\top
\end{align*}
Nous avons déjà montré que $\widehat{Q}_n ^{-1} \limd Q^{-1}$(c.f., \eqref{eq2}). Considérons maintenant $\widehat{\Omega}_n$. Nous pouvons écrire ici,
\begin{align*}
 \widehat{U}_i  = U_i - X_i(\widehat{\beta}_n - \beta)
\end{align*}
Il en résulte que,
\begin{align}
n^{-1}\sumobs \widehat{U}_i^2 X_iX_i^\top = n^{-1} \sumobs U_i^2X_iX_i^\top - 2R_{1,n} + R_{2,n}
\label{eq5}
\end{align}
où,
\begin{align*}
R_{1,n} = n^{-1} \sumobs \left((\widehat{\beta}_n - \beta)X_iU_i\right)X_iX_i^\top \ \  , \ \ 
R_{2,n} = n^{-1} \sumobs \left((\widehat{\beta}_n - \beta)X_i\right)^2X_iX_i^\top
\end{align*}
Sous les hypothèses du théorème \ref{th2}, $\Er(U_i^2X_iX_i^\top)$ est finie, comme cela a été montré dans la démonstration du théorème. Par conséquent, par la loi faible des grand nombres,
\begin{align*}
n^{-1} \sumobs U_i^2 X_iX_i^\top \limp \Er(U^2X_iX_i^\top)
\end{align*}
En outre, il est possible de montrer que $R_{1,n}$ et $R_{2,n}$ convergent en probabilité vers zéro(c.f., annexe) de sorte que,
\begin{align*}
\widehat{V}_n \limp V
\end{align*}
L'estimateur de la matrice des variances-covariances $\widehat{V}_n = \widehat{Q}_n^{-1} \widehat{\Omega}_n  \widehat{Q}_n^{-1}$, qui est ainsi donné par une formule "en sandwich" est un estimateur convergent que les termes d'erreur soient homoscédastiques ou hétéroscédastiques. Il est fréquent de l'appeler \emph{estimateur convergent robuste à l'hétéroscédasticité}, ou \emph{estimateur robuste de White}(car il fut suggéré par \citep{White1980})

\section{Intervalles de confiance asymptotiques}
Dans cette section nous nous intéressons aux intervalles de confiance pour les éléments de $\beta$. Considérons l'intervalle de confiance suivant pour $\beta_k$, $k=1,...,K$,
\begin{align*}
\CI_{n, k, 1-\alpha} = \left[\widehat{\beta}_{n, k} - z_{1-\alpha/2}\sqrt{\left[\widehat{V}_n\right]_{k,k} \Big{/} n}, 
\widehat{\beta}_{n, k} + z_{1-\alpha/2}\sqrt{\left[\widehat{V}_n\right]_{k,k} \Big{/} n}\right]
\end{align*}
où $ z_{1-\alpha/2}$ est le quantile $1-\alpha/2$ de la distribution normale standard et $\left[\widehat{V}_n\right]_{k,k} $ est l'élément $(k, k)$ de la matrice $\widehat{V}_n$. Nous allons montrer que $\Pr\left(\beta_k \in \CI_{n, k, 1-\alpha} \right) \rightarrow 1 - \alpha$ lorsque $n\rightarrow \infty$. Comme $n^{1/2}(\widehat{\beta}_n - \beta) \limd \mathcal{N}(0, V)$, et $\widehat{V}_n \limp V$, il résulte du théorème de convergence Slutsky et de celui de Cramer que,
\begin{align*}
\widehat{V}_n^{-1/2} n^{1/2}(\widehat{\beta}_n - \beta) \limd V^{-1/2}\mathcal{N}(0, V) = \mathcal{N}(0, \Id_K)
\end{align*}
et par conséquent,
\begin{align*}
\frac{\sqrt{n}(\widehat{\beta}_{n, k} - \beta)}{\sqrt{\left[\widehat{V}_n\right]_{k,k} }} \limd \mathcal{N}(0, 1)
\end{align*}
ce qui peut aussi s'écrire comme,
\begin{align*}
\Pr\left(\frac{\sqrt{n}(\widehat{\beta}_{n,k} - \beta)}{\sqrt{\left[\widehat{V}_n\right]_{k,k} }} \leq z \right) \rightarrow \Pr(Z\leq z) \ \ \textrm{pour tout} \ \ z \in \mathbb{R},
\end{align*}
où $Z$ est une variable aléatoire et $Z \sim \mathcal{N}(0, 1)$. A présent,
\begin{align*}
\Pr(\beta_k \in \CI_{n, k, 1-\alpha} ) = \Pr\left(\frac{\sqrt{n}(\widehat{\beta}_{n,k} - \beta)}{\sqrt{\left[\widehat{V}_n\right]_{k,k} }} \leq z_{1-\alpha/2} \right) \rightarrow \Pr(\abs{Z} \leq z_{1-\alpha/2}  ) = 1 - \alpha
\end{align*}
Considérons, par exemple, le cas avec des termes d'erreur homoscédastiques.  Nous avons vu que dans ce cas $\sqrt{n}(\widehat{\beta}_n - \beta) \limd \mathcal{N}\left(0, \sigma^2\left(\Er(XX^\top)\right)^{-1}\right)$. Comme $s^2 \limp \sigma^2$, la matrice des variances-covariances peut être estimée par $s^2\left(\sumobs X_iX_i^\top\right)^{-1}$. Et l'intervalle de confiance pour $\beta_k$ est alors,
\begin{align*}
\left[\widehat{\beta}_{n, k} \pm  z_{1 -  \alpha/2} \sqrt{ \left[s^2\left(n^{-1}\sumobs X_iX_i^\top\right)^{-1} \right]_{k,k} \Big{/} n}\right] = \left[\widehat{\beta}_{n, k}  \pm z_{1-\alpha/2} \sqrt{\left[s^2\left(\mathbf{X}^\top\mathbf{X}\right)\right]_{k,k}}\right] 
\end{align*}
qui est le même intervalle de confiance que celui à distance finie, sauf qu'on utilise ici les quantiles de la distribution normale standard plutôt que ceux de la loi de student.

\section{Tests d'hypothèses}
Dans cette section nous considérons les tests asymptotiques de l'hypothèse $H_0: h(\beta) = 0$ contre l'alternative 
 $H_1: h(\beta) \neq 0$, où $h: \mathbb{R}^K \mapsto \mathbb{R}^q$ est une fonction continument dérivable dans un voisinage de $\beta$. La contrainte sous $H_0$ inclut le cas des contraintes linéaires de la forme $h(\beta) = \mathbf{R}\beta - r$, où $\mathbf{R}$ est une matrice $q\times K$ et $r$ est un vecteur de taille $q$.\\ 
Considérons la \emph{statistique de test de Wald},
\begin{align*}
 W_n = nh(\widehat{\beta}_n)^\top \left(\widehat{\asyvar}\left(h(\widehat{\beta}_n)\right)\right)^{-1}h(\widehat{\beta}_n) = n h(\widehat{\beta}_n)^\top\left(\frac{\delta h}{\delta \beta ^ \top}(\widehat{\beta}_n)\widehat{V}_n \frac{\delta h}{\delta \beta}(\widehat{\beta}_n)^\top \right)^{-1} h(\widehat{\beta}_n)
\end{align*}
où $\asyvar$ désigne la variance asymptotique. Le test asymptotique de taille $\alpha$ de $H_0: h(\beta) = 0$ est alors défini par la règle,
\begin{align*}
\textrm{Rejeter} \ \ H_0 \ \ \textrm{si} \ \ W_n > \chi^2_{q, 1- \alpha}
\end{align*}
où $\chi^2_{q, 1- \alpha}$ est le quantile $(1-\alpha)$ de la distribution du $\chi^2_q$. Un test s'appuyant sur  $W_n$ est dit convergent si $\Pr(W_n > \chi^2_{q, 1- \alpha} | H_1) \rightarrow 1$.
\begin{theoreme}\label{th3}
Sous les hypothèses \ref{hy1}-\ref{hy6},
\begin{enumerate}
\item\label{th3a} $\Pr(W_n > \chi^2_{q, 1- \alpha} | H_0) \rightarrow \alpha$.
\item\label{th3b} $\Pr(W_n > \chi^2_{q, 1- \alpha} | H_1) \rightarrow 1$.
\end{enumerate}
\end{theoreme}
\begin{proof} 
\begin{enumerate}
\item Comme $n^{1/2}(\widehat{\beta}_n - \beta) \limd \mathcal{N}(0, V)$ et que $h(.)$ est continue en $\beta$, sous $H_0$, et en appliquant la méthode delta,
\begin{align*}
n^{1/2}h(\widehat{\beta}_n) \limd \mathcal{N}\left(0, \frac{\delta h}{\delta \beta^\top}(\beta) V \frac{\delta h}{\delta \beta}(\beta)^\top \right)
\end{align*}
En outre,
\begin{align*}
\frac{\delta h}{\delta \beta^\top}(\widehat{\beta}_n) \limp \frac{\delta h}{\delta \beta^\top}(\beta) \ \ \textrm{et}, \ \ \widehat{V}_n \limp V
\end{align*}
Par le théorème de convergence de Cramer, sous $H_0$,
\begin{align*}
\left(\frac{\delta h}{\delta \beta ^ \top}(\widehat{\beta}_n)\widehat{V}_n \frac{\delta h}{\delta \beta}(\widehat{\beta}_n)^\top \right)^{-1/2}n^{1/2} h(\widehat{\beta}_n) 
\limd & \left(\frac{\delta h}{\delta \beta^\top}(\beta) V \frac{\delta h}{\delta \beta}(\beta)^\top\right)^{-1/2} \mathcal{N}\left(0, \frac{\delta h}{\delta \beta^\top}(\beta) V \frac{\delta h}{\delta \beta}(\beta)^\top\right)\\
= & \mathcal{N}\left(0, \Id_q \right)
\end{align*}
Et par le théorème des applications continues, sous $H_0$,
\begin{align*}
W_n \limd \chi^2_q
\end{align*}
ce qui complète la démonstration du point \ref{th3a} du théorème.
\item Sous l'hypothèse alternative, $h(\beta)\neq 0$, par le théorème de Slustsky,
\begin{align*}
h(\widehat{\beta}_n \limp h(\beta) \neq 0
\end{align*}
Par conséquent, 
\begin{align*}
W_n /n \limp h(\beta)^\top\left(\frac{\delta h}{\delta \beta^\top}(\beta) V \frac{\delta h}{\delta \beta^\top}(\beta)^\top \right)^{-1} h(\beta)
\end{align*}
et par conséquent, sous $H_1$, 
\begin{align*}
W_n \rightarrow \infty
\end{align*}
\end{enumerate}
\end{proof}
Remarquons que dans le cas de contraintes linéaires $h(\beta) = R\beta - r$, nous avons:
\begin{align*}
W_n = n\left(R\widehat{\beta}_n - r\right)^\top\left(R\widehat{V}_nR^\top\right)\left(R\widehat{\beta}_n - r\right)
\end{align*}
En outre dans le cas homoscédastique, on peut remplacer $\widehat{V}_n$ par $s^2(\mathbf{X}^\top\mathbf{X}/n)^{-1}$. Alors, la statistique de Wald devient,
\begin{align*}
W_n = \left(R\widehat{\beta}_n - r\right)^\top\left(s^2R(\mathbf{X}^\top\mathbf{X})^{-1}R^\top\right)^{-1}\left(R\widehat{\beta}_n-r\right)
\end{align*}
qui est similaire à l'expression de la statistique de Fisher, mis à part l'ajustement relatif au nombre de degrés de liberté dans le numérateur.


\appendix

\section{Convergence de l'estimateur de la matrice des variances-covariances(suite)}
Dans cette annexe nous montrons que les termes $R_{1,n}$ et $R_{2,n}$ de l'équation \eqref{eq5} convergent en probabilité vers zéro. La démonstration utilise le résultat suivant appelé \emph{inégalité de Holder}.
\begin{proposition}\textbf{(Inégalité de Hölder)} Soit $X$ et $Y$ deux variables aléatoires. Si $p>1$, $q>1$,  $1/p +1/q =1$,  alors $\Er(\abs{XY}) \leq \left(\Er\abs{X}^p\right)^{1/p}
\left(\Er\abs{Y}^q\right)^{1/q}$.
\end{proposition}
La convergence en probabilité vers zéro élément par élément est équivalente à la convergence en probabilité des normes vers zéro. La norme d'une matrice $A$ est donnée par,
\begin{align*}
\norm{A}& = \left(\Tr(A^\top A)\right)^{1/2}\\
&= \left(\underset{i}{\sum}\underset{j}{\sum}a_{ij}^2\right)^{1/2}
\end{align*}
où $a_{ij}$ est l'élément $(i, j)$ de la matrice $A$. Pour $R_{1,n}$,
\begin{align*}
\norm{n^{-1}\sumobs\left((\widehat{\beta}_n - \beta)^\top X_iU_i\right)X_iX_i^\top} &\leq 
n^{-1} \sumobs \norm{\left((\widehat{\beta}_n - \beta)^\top X_iU_i\right)X_iX_i^\top}\\
 &= n^{-1} \sumobs \Tr\left(U_i^2\left(\left(\widehat{\beta}_n - \beta\right)^\top X_i\right)^2 X_iX_i^\top X_iX_i^\top\right)^{1/2}\\
 &=n^{-1} \sumobs \abs{U_i}\abs{\left(\widehat{\beta}_n - \beta\right)^\top X_i}\norm{X_i}\Tr(X_iX_i^\top)^{1/2} \\
 &=n^{-1}\sumobs \abs{U_i}\abs{\left(\widehat{\beta}_n - \beta\right)^\top}\norm{X_i}^2
\end{align*}
Par l'inégalité de Cauchy-Schwartz,
\begin{align*}
\abs{(\widehat{\beta}_n - \beta)^\top X_i} \leq \norm{\widehat{\beta}_n - \beta}\norm{X_i}
\end{align*}
Par conséquent,
\begin{align*}
\norm{R_{1,n}} \leq \norm{\widehat{\beta}_n - \beta}n^{-1} \sumobs \abs{U_i}\norm{X_i}^3
\end{align*}
Par l'inégalité de Holder avec $p=4$ et $q=4/3$,
\begin{align*}
\Er\left(\abs{U_i}\norm{X_i}^3\right)&\leq (\Er(\abs{U_i}^4))^{1/4}(\Er(\norm{X_i}^4))^{3/4}\\
&< \infty
\end{align*}
étant donné que par l'hypothèse \ref{hy6} nous avons $\Er(\abs{U_i})^4<\infty$, et,
\begin{align}
\Er(\norm{X_i}^4) &= \Er\left(\underset{r=1}{\overset{K}{\sum}}X_{i, r}^2\right)^2\nonumber\\
&= \underset{r=1}{\overset{K}{\sum}}\underset{s=1}{\overset{K}{\sum}}\Er(X_{i, r}^2X_{i, s}^2)
\label{eq6}
\end{align}
où $Er(X_{i, r}^2X_{i, s}^2) < \infty$ en raison de l'hypothèse \ref{hy5}, comme cela a été montré dans le théorème \ref{th2}. Par conséquent, par la LFGN,
\begin{align*}
n^{-1} \sumobs \abs{U_i}\norm{X_i}^3 \limp \Er(\abs{U_i}\norm{X_i}^3)
\end{align*}
et comme nous avons $\norm{\widehat{\beta}_n - \beta} \limp 0$, nous avons que $R_{1,n} \limp 0$.\\
Considérons maintenant le cas de $R_{2,n}$. Par des arguments similaires aux précédents, nous pouvons borner $R_{2,n}$ par,
\begin{align*}
\norm{n^{-1} \sumobs \left((\widehat{\beta}_n-\beta)^\top X_i\right)^2 X_iX_i^\top} &\leq
n^{-1}\sumobs  \left((\widehat{\beta}_n-\beta)^\top X_i\right)^2 \norm{X_i}\Tr(X_iX_i^\top)^{1/2}\\
&= \norm{(\widehat{\beta}_n-\beta)}^2n^{-1} \sumobs\norm{X_i}^4
\end{align*}
Et par \eqref{eq6} et la LFGN,
\begin{align*}
n^{-1} \sumobs\norm{X_i}^4 \limp \Er(\norm{X_i}^4)
\end{align*}
et par conséquent, $R_{2,n} \limp 0$.

\bibliographystyle{plainnat}
\bibliography{biblio}
\end{document}