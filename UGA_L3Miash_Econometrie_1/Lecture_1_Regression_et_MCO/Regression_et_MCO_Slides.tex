%\documentclass[ignorenonframetext, compress, 9pt, xcolor=svgnames]{beamer} 
\input{../Config_slides}
 

\title[Regression linéaire]{UGA L3 Miash:\\ \'Econométrie 1\\ \textbf{Modèle de r\'egression linéaire et moindres carrés}}
\date{\today}
\author{Michal W. Urdanivia\inst{*}}
\institute{\inst{*}Universit\'e de Grenoble Alpes, Facult\'e d'\'Economie, GAEL, \\e-mail: \href{mailto:michal.wong-urdanivia@univ-grenoble-alpes.fr}{michal.wong-urdanivia@univ-grenoble-alpes.fr}}


\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Contenu}
\tableofcontents[pausesections, pausesubsections]
\end{frame}

\section{Modèle de régression linéaire et moindres carrés}
\frame{\sectionpage}

\begin{frame}[allowframebreaks]{Définitions}
\begin{itemize}
\item On s'intéresse à l'effet d'un groupe de variables $X\in \mathbb{R}^K$, traditionnellement appelées \emph{régresseurs}, sur une autre variable $Y\in\mathbb{R}$ traditionnellement  appelée \emph{variable dépendante}. \item On dispose de données $\{(Y_i, X_i)\}_{i=1}^n$ où $Y_i$ est une variable aléatoire et $X_i$ est un vecteur $K\times 1$,
\begin{align*}
X_i = 
\left(
\begin{array}{c}
X_{i1}\\
X_{i2}\\
.\\
.\\
.\\
X_{iK}
\end{array}
\right)
\end{align*}
\item $(Y_i, X_i)$ est appelée observation(sous entendu de $(Y, X)$), et l'ensemble des $n$ observations est un 
\emph{échantillon}. 
\item Le vecteur $X_i$ contient les valeurs des $K$ variables pour l'observation $i$. 
\item Pour des \emph{données en coupe}  il est souvent supposé que toutes les observations sont tirées indépendamment les unes des autres à partir d'une même distribution(i.e., loi de probabilité). 
\item On dit dans ce cas que l'échantillon d'observations $\{(Y_i, X_i)\}_{i=1}^n$ est un échantillon aléatoire ou de manière équivalente que les observations sont identiquement et indépendamment distribuées(i.i.d.).
\item L'hypothèse d'observations i.i.d. ne signifie pas que $Y_i$ et $X_i$ soient indépendants, mais plutôt que l'observation $(Y_i, X_i)$ est indépendante de toute autre observation $(Y_j, X_j)$ pour $i\neq j$, n'excluant donc pas que $Y_i$ et $X_i$ puissent être liés.
\item Un outil pour étudier la relation entre la variable dépendante et les régresseurs est l'espérance conditionnelle de $Y_i$ sachant $X_i$, $\Er(Y_i| X_i)$, laquelle vue comme une fonction de $X_i$ est appelée \emph{fonction de régression}.\\
 \item La différence entre $Y_i$ et son espérance conditionnelle(i.e., sa fonction de régression) est appelée \emph{terme d'erreur}(ou plus succinctement \emph{erreur}),
\begin{align}
U_i = Y_i - \Er(Y_i| X_i)
\label{eq1}
\end{align} 
\item $U_i$ n'est pas une variable observable par l'analyste étant donné que l'espérance conditionnelle lui est inconnue.
\item Dans un cadre \emph{paramétrique} ou \emph{semi-paramétrique}, il est souvent supposé que l'espérance conditionnelle est connue à un ensemble de \emph{paramètres} près. 
\item Ainsi dans le modèle de régression linéaire on suppose que $\Er(Y_i|X_i)$ est linéaire par rapport à un vecteur de paramètres inconnus,
\begin{align}
\Er(Y_i|X_i) = X_{i1}\beta_1 +  X_{i2}\beta_2 + ...+ X_{iK}\beta_K = X_i^\top\beta
\label{eq2}
\end{align}

où,

\begin{align*}
 \beta = \left(
 \begin{array}{c}
 \beta_1\\
 \beta_2\\
 .\\
 .\\
 .\\
 \beta_K
 \end{array}
 \right)
\end{align*}
est un vecteur de $K$ paramètres constants. 
\item La linéarité de $\Er(Y_i|X_i)$ peut être justifiée, si par exemple, la distribution des observations $\{(Y_i, X_i)\}_{i=1}^n$ est une loi normale multivariée. 
\item Rappelons néanmoins que lorsque $\Er(Y_i|X_i)$ n'est pas linéaire il est possible de caractériser $\beta$ de manière à ce que \eqref{eq2} constitue la \emph{meilleure prédiction linéaire} de la variable dépendante par les régresseurs. 
\item Notons aussi que comme
\begin{align*}
\beta_k = \frac{\delta \Er(Y_i|X_i)}{\delta X_{ik}}, \ \ k=1,2,...,K.
\end{align*}
le vecteur $\beta$ est le vecteur des \emph{effets marginaux} des régresseurs, i.e., $\beta_k$ donne la variation dans l'espérance  conditionnelle de $Y_i$ lorsque le régresseur $X_{ik}$ varie, pour des valeurs fixes des autres régresseurs $X_{il}$, $l=1,2,...,K$, $l\neq k$. 
\item Ceci est une des raisons pour lesquelles un des principaux objectifs est l'estimation du vecteur inconnu $\beta$ à partir des données.
\item Les équations \eqref{eq1} et \eqref{eq2} permettent d'écrire,
\begin{align}
Y_i = X_i^\top\beta +U_i
\label{eq3}
\end{align}
où par définition de \eqref{eq1}
\begin{align}
\Er(U_i|X_i) = 0
\label{eq4}
\end{align}
\item Ceci implique que les régresseurs ne contiennent aucune information quant à l'écart entre $Y_i$ et sont espérance conditionnelle. 
\item En outre, \emph{la loi des conditionnements successifs} implique que les erreurs ont une espérance nulle: $\Er(U_i) = 0$. Notons aussi qu'avec des observations i.i.d. les erreurs sont aussi i.i.d.
\item Une hypothèse fréquente sur les erreurs consiste à supposer qu'ils sont \emph{homoscédastiques}(on parle d'hypothèse d'homoscédasticité), par quoi on entend que leur variance est indépendante des régresseurs, et la même pour toutes les observations,
\begin{align*}
\Var(U_i|X_i) = \sigma^2
\end{align*}
pour une constante $\sigma^2>0$.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Hypothèses}
\begin{itemize}
\item  Introduisons les notations vectorielles et matricielles suivantes,

\begin{align*}
\mathbf{Y}=\left(
\begin{array}{c}
Y_1\\
Y_2\\
.\\
.\\
.\\
Y_n
\end{array}
\right)
\end{align*}
\begin{align*}
\mathbf{X}=\left(
\begin{array}{c}
X_1^\top\\
X_2^\top\\
.\\
.\\
.\\
X_n^\top
\end{array}
\right)
=
\left(
\begin{array}{cccccc}
X_{11} & X_{12}&.&.&.&X_{1K}\\
X_{21} & X_{22}&.&.&.&X_{2K}\\
.\\
.\\
.\\
X_{n1} & X_{n2}&.&.&.&X_{nK}
\end{array}
\right)
\end{align*}
\begin{align*}
\mathbf{U}=\left(
\begin{array}{c}
U_1\\
U_2\\
.\\
.\\
.\\
U_n
\end{array}
\right)
\end{align*}
\item Le modèle de régression linéaire consiste dans les hypothèses suivantes:
\begin{hypothese}
$\mathbf{Y} = \mathbf{X}\beta +\mathbf{U}$
\label{hy1}
\end{hypothese}
\begin{hypothese}
$\Er(\mathbf{U} | \mathbf{X}) = 0$ p.s.
\label{hy2}
\end{hypothese}
\begin{hypothese}
$\Var(\mathbf{U}|\mathbf{X}) = \sigma^2\Id_n$ p.s.
\label{hy3}
\end{hypothese}
\begin{hypothese}
$\Rang(\mathbf{X}) = K$ p.s.
\label{hy4}
\end{hypothese}
\item Pour l'inférence nous supposerons parfois que,
\begin{hypothese}
$\mathbf{U} | \mathbf{X} \sim \mathcal{N}(\mathbf{0}, \Id_n)$
\label{hy5}
\end{hypothese}
\item Les hypothèses \ref{hy1}-\ref{hy5} définissent alors le \emph{modèle de régression linéaire normal} avec,
\begin{align*}
\mathbf{Y}|\mathbf{X} \sim\mathcal{N}(\mathbf{X}\beta, \sigma^2\Id_n)
\end{align*}
\item Remarquons qu'étant donné que les covariances dans \ref{hy5} sont toutes nulles,  \ref{hy5} implique l'indépendance des erreurs. 
\item Les hypothèses \ref{hy1}-\ref{hy4} seules, n'impliquent pas l'indépendance entre les observations. 
\item En fait, plusieurs résultats importants n'exigent pas d'observations indépendantes. 
\item Néanmoins, nous supposerons parfois l'indépendance  sans la normalité.
\begin{hypothese}
Les observations $\{(Y_i, X_i)\}_{i=1}^n$ sont i.i.d..
\label{hy6}
\end{hypothese}
Dans le cas de régresseurs fixes cette hypothèse peut être remplacé par celle d'erreurs i.i.d.
\item L'hypothèse \ref{hy2} dit que $\mathbf{U}$ est indépendant de $\mathbf{X}$ en espérance, ce qui est une hypothèse forte. 
\item Cependant, plusieurs résultats importants peuvent être obtenus avec une hypothèse plus faible  de non corrélation.
\begin{hypothese}
Pour $i=1,2,...,n$, $\Er(U_iX_i)=0$, et $\Er(U_i)=0$.
\label{hy7}
\end{hypothese}
\item Toutefois sous cette condition $X_i^\top\beta$ ne peut pas s'interpréter comme une espérance conditionnelle, auquel cas \eqref{eq3} doit être vu comme un \emph{processus générateur des données}.
\item L'hypothèse \ref{hy3} implique que les erreurs $U_i$ ont la même variance pour tout $i$, et ne sont pas corrélés entre eux, i.e., $\Er(U_iU_j| \mathbf{X}) = 0$ pour $i\neq j$. 
\item Notons que l'indépendance entre les erreurs peut aussi être obtenue avec la condition \ref{hy5} ou sous les  conditions \ref{hy1} et \ref{hy6}.\\
\item L'hypothèse \ref{hy4} exige que le colonnes de $\mathbf{X}$ soient linéairement indépendantes. 
\item Que cette hypothèse ne soit pas vérifiée signifie qu'un ou plus de régresseurs duplique l'information contenue dans les autres, et ce faisant doit être écarté.
\item Souvent, une des colonnes de $\mathbf{X}$ est le vecteur unitaire et le paramètre qui lui est associé est appelé \emph{constante}. 
\item La constante du modèle donne la valeur moyenne de la variable dépendante lorsque tous les régresseurs sont égaux à zéro.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Estimation par la méthode des moments}
\begin{itemize}
\item Nous allons construire des estimateurs des paramètres $\beta$ et $\sigma^2$. 
\item Une des méthodes les plus ancienne pour construire des estimateurs est la \emph{méthode des moments}(MM). \item La MM consiste à construire des estimateurs pour des paramètres définis par des moments théoriques en considérant les contreparties empiriques de ces moments appelées alors moments empiriques. 
\item Par exemple si un paramètre est défini au travers d'une espérance(moment théorique), son  estimateur sera construit à partir d'une moyenne(moment empirique) calculée sur les observations.
\item Les hypothèses \ref{hy1}, et \ref{hy2} ou \ref{hy7} impliquent que la vraie valeur de $\beta$ doit satisfaire,
\begin{align}
\Er(U_iX_i) = \Er\left((Y_i-X_i^\top\beta)X_i\right) = 0
\label{eq5}
\end{align}
 \item Un \emph{estimateur des moments}(i.e., obtenu selon la MM) de $\beta$, $\widehat{\beta}$,  est obtenu en remplaçant l'espérance dans \eqref{eq5} par la moyenne empirique,
\begin{align}
n^{-1}\sumobs(Y_i - X_i^\top\widehat{\beta})X_i = n^{-1}\sumobs X_iY_i - n^{-1}\sumobs X_iX_i^\top\widehat{\beta} = 0
\label{eq6}
\end{align}
\item En résolvant par rapport à $\widehat{\beta}$ on obtient,
\begin{align}
\widehat{\beta} = \left(n^{-1}\sumobs X_iX_i^\top\right)^{-1}n^{-1}\sumobs X_iY_i
\label{eq7}
\end{align}
qui peut s'écrire alternativement,
\begin{align}
\widehat{\beta} = \left(\sumobs X_iX_i^\top\right)^{-1}\sumobs X_iY_i
\label{eq8}
\end{align}
ou,
\begin{align}
\widehat{\beta} = \left(\mathbf{X}^\top\mathbf{X}\right)^{-1}\mathbf{X}^\top\mathbf{Y}
\label{eq9}
\end{align}
où l'on note que la matrice $\sumobs X_iX_i^\top = \mathbf{X}^\top\mathbf{X}$ est inversible sous l'hypothèse \ref{hy4}.
\item On définit les \emph{valeurs ajustées} ou \emph{prédictions} , ainsi qu'un vecteur $n\times 1$ des valeurs ajustées ou des prédictions, par respectivement,
\begin{align*}
\widehat{Y}_i = X_i^\top\widehat{\beta}, \ \ \widehat{\mathbf{Y}} = (\widehat{Y}_1,  \widehat{Y}_2,..., \widehat{Y}_n)^\top
\end{align*}
\item On définit  les \emph{résidus}  , et le vecteur  $n\times 1$ des résidus, par respectivement,
\begin{align*}
\widehat{U}_i = Y_i - X_i^\top\widehat{\beta}, \ \ \widehat{\mathbf{U}} = (\widehat{U}_1,  \widehat{U}_2,..., \widehat{U}_n)^\top
\end{align*}
\item Du fait de \eqref{eq6} le vecteur des résidus vérifie les $K$  \emph{équations normales},
\begin{align}
\sumobs \widehat{U}_iX_i=
\left(
\begin{array}{c}
\sumobs \widehat{U}_iX_{i1}\\
\sumobs \widehat{U}_iX_{i2}\\
.\\
.\\
.\\
\sumobs \widehat{U}_iX_{iK}
\end{array}
\right)
=0
\label{eq10}
\end{align}
ou en notation matricielle,
\begin{align}
\mathbf{X}^\top\widehat{\mathbf{U}} = 0
\label{eq11}
\end{align}
\item Si le modèle contient une constante alors il résulte des équations normales que $\sumobs \widehat{U}_i = 0$(il suffit en effet de considérer que, par exemple, le premier régresseur est constant et égal à 1).
\item Afin d'estimer $\sigma^2$ considérons,
\begin{align*}
\sigma^2 = \Er(U_i^2)=\Er\left((Y_i - X_i^\top\beta)^2\right)
\end{align*}
\item Dans la mesure où $\beta$, est inconnu un estimateur  sera obtenu en remplaçant $\beta$ par sont estimateur des moments,
\begin{align}
\widehat{\sigma}^2 = n^{-1}\sumobs(Y_i-X_i^\top\widehat{\beta})^2
\label{eq12}
\end{align}
\end{itemize}
\end{frame}
\begin{frame}[allowframebreaks]{Moindres carrés}
\begin{itemize}
\item Soit le problème consistant à minimiser l'erreur de prédiction quand on cherche à prédire $Y_i$ par son espérance conditionnelle, $\Er(Y_i|X_i)$, supposée être une fonction linéaire telle que \eqref{eq2}. 
\item Plus précisément, $Y_i - \Er(Y_i|X_i)$ étant l'erreur de prédiction  on cherche $\beta$ qui minimise un critère de perte quadratique,
\begin{align*}
\beta \in \argmin_{b\in \mathbb{R}^K}S(b)
\end{align*}
où $S(b) =\Er\left((Y_i - X_i^\top b)^2\right)$.
\item La contrepartie empirique de ce problème permet de définir un estimateur de $\beta$ par,
\begin{align*}
\widehat{\beta} \in \argmin_{b\in \mathbb{R}^K}S_n(b)
\end{align*}
où $S_n(b)=n^{-1}\sumobs\left((Y_i - X_i^\top\beta)^2\right)$, est la contrepartie empirique de la fonction objectif $S(b)$.
\item Nous pouvons montrer que l'estimateur des moments de la section précédente est aussi l'estimateur des moindres carrés.
\item  La fonction objectif précédente peut s'écrire(voir notes),
\begin{align*}
S_n(b) = &(\mathbf{Y} - \mathbf{X}\widehat{\beta})^\top(\mathbf{Y} - \mathbf{X}\widehat{\beta}) + (\widehat{\beta}-b)^\top \mathbf{X}^\top\mathbf{X} (\widehat{\beta}-b) 
\end{align*}
\item La minimisation de $S_n(b)$ équivaut à minimiser $(\widehat{\beta}-b)^\top \mathbf{X}^\top\mathbf{X} (\widehat{\beta}-b)$ car $(\mathbf{Y} - \mathbf{X}\widehat{\beta})^\top(\mathbf{Y} - \mathbf{X}\widehat{\beta})$ ne fait pas intervenir $b$.  
\item Sous l'hypothèse \ref{hy4} la matrice $\mathbf{X}$ est de plein rang, et dans ce cas $\mathbf{X}^\top\mathbf{X}$ est définie positive,
\begin{align*}
(\widehat{\beta}-b)^\top \mathbf{X}^\top\mathbf{X} (\widehat{\beta}-b) \geq 0
\end{align*}
et $(\widehat{\beta}-b)^\top \mathbf{X}^\top\mathbf{X} (\widehat{\beta}-b) = 0$ ssi $\widehat{\beta} = b$.
\item Alternativement, nous pouvons montrer que $\widehat{\beta}=(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}$ est l'estimateur des moindres carrés de $\beta$(i.e., il minimise $S_n(b)$). 
\item Pour cela, écrivons,
\begin{align*}
S(b) = \mathbf{Y}^\top\mathbf{Y} - 2b^\top\mathbf{X}^\top\mathbf{Y}+b^\top\mathbf{X}^\top\mathbf{X}b
\end{align*}
\item En utilisant le fait que pour une matrice symétrique $\mathbf{A}$,
\begin{align*}
\frac{\delta (x^\top\mathbf{A}x)}{\delta x} = 2\mathbf{A}x
\end{align*}
la condition du premier ordre est,
\begin{align*}
\frac{\delta S_n(\widehat{\beta})}{\delta b} = -2\mathbf{X}^\top\mathbf{Y} + 2\mathbf{X}^\top\mathbf{X}\widehat{\beta} = 0
\end{align*}
ce qui permet d'obtenir,
\begin{align*}
\widehat{\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}
\end{align*}
\item Remarquons aussi que les conditions du premier ordre peuvent s'écrire $\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\widehat{\beta}) = 0$, ce qui correspond aux équations normales vue précédemment.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Propriétés de l'estimateur des moindres carrés}
\begin{itemize}
\item Nous allons présenter un certain nombre de propriétés de l'estimateur des moindres carrés.
\begin{proposition}$\widehat{\beta}$ est un estimateur linéaire.
\end{proposition}

\item Un estimateur $b$ est linéaire s'il peut s'écrire comme $b=\mathbf{A}\mathbf{Y}$, où $\mathbf{A}$ est une matrice quelconque qui dépend de $\mathbf{X}$ uniquement, et ne dépend pas de $\mathbf{Y}$.
\item  Pour l'estimateur des moindres carrés nous avons, $\mathbf{A} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$.

\begin{proposition}
Sous les hypothèses \ref{hy1}, \ref{hy2}, et \ref{hy4}, $\widehat{\beta}$ est sans biais, i.e.,
\begin{align*}
\Er(\widehat{\beta}) = \beta
\end{align*}
\end{proposition}

\item Pour montrer cette propriété écrivons, en utilisant l'hypothèse \ref{hy1},
\begin{align*}
\widehat{\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y} =  (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top (\mathbf{X}\beta + \mathbf{U}) = 
\beta + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U}
\end{align*}
Calculons l'espérance conditionnelle de $\widehat{\beta}$,
\begin{align*}
\Er\left(\widehat{\beta} | \mathbf{X}\right) = \Er\left(\beta + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U} | \mathbf{X}\right) = \beta +\ \Er\left((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U} | \mathbf{X}\right)
\end{align*}
Notons que,
\begin{align*}
\Er\left((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U} | \mathbf{X}\right) = 
\left(\mathbf{X}\mathbf{X}\right)^{-1}\mathbf{X}^\top\Er(\mathbf{U}|\mathbf{X}) = 0
\end{align*} 
car sous l'hypothèse \ref{hy2}, $\Er(\mathbf{U}|\mathbf{X}) = 0$. Nous avons donc,
\begin{align}
\Er\left(\widehat{\beta} | \mathbf{X}\right) = \beta
\label{eq13}
\end{align}
et par la loi des espérances itérées,
\begin{align*}
\Er\left(\widehat{\beta}\right) = \Er\left(\Er\left(\widehat{\beta} | \mathbf{X}\right)\right)=\beta
\end{align*}

\item L'équation \eqref{eq13} montre que $\widehat{\beta}$ est conditionnellement sans biais sachant $\mathbf{X}$. \item On remarque aussi que pour que $\widehat{\beta}$ soit sans biais l'hypothèse \ref{hy7} n'est pas suffisante.
\begin{proposition}
Sous les hypothèses \ref{hy1}, \ref{hy2}, et \ref{hy4},
\begin{align*}
\Var\left(\widehat{\beta} | \mathbf{X}\right) = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\Er\left(\mathbf{U}\mathbf{U}^\top | \mathbf{X}\right)\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}
\end{align*}
et avec des erreurs homoscédastiques(i.e., sous l'hypothèse \ref{hy3}),
\begin{align*}
\Var\left(\widehat{\beta} | \mathbf{X}\right) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\end{align*}
\end{proposition}
\item Pour montrer ces résultats, partons de la définition de la variance conditionnelle de $\widehat{\beta}$,
\begin{align*}
\Var\left(\widehat{\beta} | \mathbf{X}\right) =& \Er\left(\left(\widehat{\beta} - \Er(\widehat{\beta} | \mathbf{X})\right) \left(\widehat{\beta} - \Er(\widehat{\beta} | \mathbf{X})\right)^\top | \mathbf{X}\right)\\
=&\Er\left(\left(\widehat{\beta} - \beta\right) \left(\widehat{\beta} - \beta\right) ^\top | \mathbf{X}\right)\\
=&\Er\left((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U}\mathbf{U}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}|\mathbf{X}\right)\\
=&(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\Er\left(\mathbf{U}\mathbf{U}^\top | \mathbf{X}\right)\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}
\end{align*}
\item Et avec des erreurs homoscédastiques, $\Er(\mathbf{U}\mathbf{U}^\top| \mathbf{X}) = \sigma^2\Id_n$, de sorte que,
\begin{align*}
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\Er\left(\mathbf{U}\mathbf{U}^\top | \mathbf{X}\right)\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1} =& (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \sigma^2\Id_n \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\\
=& \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\\
=&\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\end{align*}
\item Notons qu'avec des régresseurs fixes $\Var\left(\widehat{\beta} | \mathbf{X}\right) =  \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}$.

\begin{proposition}
Sous les hypothèses \ref{hy1} - \ref{hy5},
\begin{align*}
\widehat{\beta} | \mathbf{X} \sim \mathcal{N}\left(\beta, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)
\end{align*}
\end{proposition}
\item  Il est suffit ici de montrer ici que conditionnellement à $\mathbf{X}$ la distribution de $\widehat{\beta}$ est normale.
\item  On aura alors que, $\widehat{\beta} | \mathbf{X}\sim \mathcal{N}\left(\Er(\widehat{\beta}| \mathbf{X}), \Var(\widehat{\beta} | \mathbf{X})\right)$. 
\item Néanmoins la normalité de $\widehat{\beta} | \mathbf{X}$ résulte ici de ce que $\widehat{\beta}$ est une fonction de linéaire de $\mathbf{Y}$, et que sous l'hypothèse \ref{hy5} $\mathbf{Y}|\mathbf{X}$ est normale.
\item Dans le cas de régresseur fixes, il suffit d'omettre le conditionnement par rapport à $\mathbf{X}$ et,
\begin{align*}
\widehat{\beta}\sim\mathcal{N}\left(\beta, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)
\end{align*}
\begin{proposition}(\textbf{\'Efficacité ou théorème de Gauss-Markov.}) Sous les hypothèses \ref{hy1}-\ref{hy4}, l'estimateur des moindres carrés est le meilleur estimateur linéaire sans biais de $\beta$, dans le sens où il s'agit de l'estimateur, dans la classe des estimateurs linéaires et sans biais, qui présente la plus petite variance. i.e., pour tout estimateur linéaire sans biais, $b$, la matrice $\Var\left(b|\mathbf{X}\right)-\Var\left(\widehat{\beta}|\mathbf{X}\right)$ doit être semi-définie positive:
\begin{align*}
\Var\left(b|\mathbf{X}\right)-\Var\left(\widehat{\beta}|\mathbf{X}\right) \geq 0
\end{align*}
En outre, si $\tilde{\beta}$ est un estimateur linéaire et sans biais et 
$\Var\left(\tilde{\beta}|\mathbf{X}\right) = \Var\left(\widehat{\beta}|\mathbf{X}\right)$, alors $\tilde{\beta} = \widehat{\beta}$ p.s.
\end{proposition}
\item Avant de démontrer ce résultat notons qu'il discute la variance conditionnelle de l'estimateur des moindres carrés, et ce faisant il se réfère à des estimateurs conditionnellement sans biais.
\item Soit $b$ un estimateur linéaire sans biais de $\beta$. Il doit ainsi vérifier,
\begin{align*}
b=\mathbf{A}\mathbf{Y}, \ \ \ \Er(b|\mathbf{X}) = \beta
\end{align*} 
\item Ces deux conditions impliquent que $\mathbf{A}\mathbf{X} =\Id_K$ p.s. En effet,
\begin{align*}
\Er(b | \mathbf{X}) = &\Er\left(\mathbf{A}\left(\mathbf{X}\beta + \mathbf{U}\right)\right)\\
=&\mathbf{A}\mathbf{X}\beta + \mathbf{A}\Er(\mathbf{U}| \mathbf{X})
\end{align*}
\item Par l'hypothèse \ref{hy2}, $\Er(\mathbf{U} | \mathbf{X}) = 0$, et par conséquent, pour que $b$ soit sans biais nous avons besoin de $\mathbf{A}\mathbf{X} = \Id_K$.
\item Montrons maintenant que $\Cov\left(\widehat{\beta}, b | \mathbf{X}\right) = \Var\left(\widehat{\beta} | \mathbf{X}\right)$,
\begin{align*}
\Cov\left(\widehat{\beta}, b | \mathbf{X}\right) =& \Er\left((\widehat{\beta} - \beta)(b-\beta)^\top\right)\\
=&\Er\left((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{U}\mathbf{U}^\top\mathbf{A}^\top | \mathbf{X}\right)\\
=&(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\Er(\mathbf{U}\mathbf{U}^\top|\mathbf{X})\mathbf{A}^\top)\\
=&\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{A}^\top(\textrm{car sous \ref{hy3}, \  } \Er(\mathbf{U}\mathbf{U}^\top| \mathbf{X}) = \sigma^2\Id_n)\\
=&\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}(\textrm{car, \ } \mathbf{X}^\top\mathbf{A}^\top = \Id_K)\\
=&\Var\left(\widehat{\beta}| \mathbf{X}\right)
\end{align*}
Finalement, 
\begin{align}
\Var\left(\widehat{\beta} - b | \mathbf{X}\right) =&\Var\left(\widehat{\beta}| \mathbf{X}\right) - \Cov\left(\widehat{\beta}, b | \mathbf{X}\right) - \Cov\left(b, \widehat{\beta} | \mathbf{X}\right) + \Var\left(b\mathbf{X}\right)\nonumber\\
=&   \Var\left(b | \mathbf{X}\right) - \Var\left(\widehat{\beta}| \mathbf{X}\right)
\label{eq14}
\end{align}
et notons que dans la mesure où toute matrice de variance-covariances est semi-définie positive, nous avons,
\begin{align*}
\ \Var\left(b | \mathbf{X}\right) - \Var\left(\widehat{\beta}| \mathbf{X}\right) \geq 0
\end{align*}
\item Pour démontrer l'unicité, considérons un estimateur linéaire sans biais $\tilde{\beta}$ tel que $\Var\left(\tilde{\beta}|\mathbf{X}\right)=\Var\left(\widehat{\beta}|\mathbf{X}\right)$. 
\item Alors, par \eqref{eq14}, $\Var\left(\widehat{\beta} - b | \mathbf{X}\right) = 0$, et par conséquent, $\tilde{\beta} = \widehat{\beta}+c(\mathbf{X})$ pour une fonction $c(\mathbf{X})$ à valeurs dans $\mathbb{R}^K$ qui dépend uniquement de $\mathbf{X}$. 
\item Cependant, comme $\widehat{\beta}$ et $\tilde{\beta}$ sont conditionnellement sans biais sachant $\mathbf{X}$, il s'en suit que $c(\mathbf{X}) = 0$  p.s.
\item Notons que l'hypothèse \ref{hy3}, $\Er(\mathbf{U}\mathbf{U}^\top|\mathbf{X})=\sigma^2\Id_n$, joue un rôle crucial dans la démonstration du résultat précédent.
\item Sans elle, il ne serait pas possible de tirer des conclusions quant à  l'efficacité de l'estimateur des moindres carrés.
\end{itemize}
\end{frame}

\section{Géométrie des moindres carrés}
\frame{\sectionpage}
\begin{frame}[allowframebreaks]{Matrices de projection}
\begin{itemize}
\item Nous pouvons penser à $\mathbf{Y}$ et aux colonnes $\mathbf{X}$
  comme des éléments de l'espace euclidien à $n$ dimensions,
  $\mathbb{R}^n$. 

\item Considérons  le sous-espace de $\mathbb{R}^n$
  appelé l'\emph{espace des colonnes} de la matrice $n\times K$,
  $\mathbf{X}$. 
\item C'est la collection de tous les vecteurs dans $\mathbb{R}^n$ qui peuvent s'écrire comme des combinaisons linéaires des colonnes de $\mathbf{X}$,
\begin{align*}
\mathcal{S}(\mathbf{X}) = \left\{z \in \mathbb{R}^n: z = \mathbf{X}b, b = (b_1, b_2,...,b_K) \in \mathbb{R}^K  \right\}
\end{align*}
\item Rappelons maintenant qu'\'etant donné deux vecteurs $a$, $b$, dans $\mathbb{R}^n$, la
  distance entre $a$ et $b$ est donné par la norme
  euclidienne  de leur différence $||a-b|| =
  \sqrt{(a-b)^\top(a-b)}$. 
\item Par conséquent, le problème de  la minimisation de la somme des
  carrés des erreurs,
  $(\mathbf{Y}-\mathbf{X}b)^\top(\mathbf{Y}-\mathbf{X}b)$, 
  consiste à trouver, parmi tous les éléments de $\mathcal{S}(\mathbf{X})$, celui dont la distance par rapport à $\mathbf{Y}$ est la plus petite,
\begin{align*}
\underset{\tilde{\mathbf{Y}}\in \mathcal{S}(\mathbf{X})}{\min} ||\mathbf{Y} - \tilde{\mathbf{Y}}||^2
\end{align*}
\item Une solution au problème des moindres carrés, $\widehat{Y} = \mathbf{X}\widehat{\beta}$ doit être choisie de sorte que le vecteur des résidus, $\widehat{\mathbf{U}} = \mathbf{Y}-\widehat{\mathbf{Y}}$ soit orthogonal(perpendiculaire) à chaque colonne de $\mathbf{X}$,
\begin{align*}
\widehat{\mathbf{U}}^\top\mathbf{X} = 0
\end{align*}
\item Un  résultat de cela est que $\widehat{\mathbf{U}}$ est orthogonal à chaque élément de $\mathcal{S}(\mathbf{X})$: si $z\in \mathcal{S}(\mathbf{X})$, alors il existe $b\in\mathbb{R}^K$ tel que $z=\mathbf{X}b$, et,
\begin{align*}
\widehat{\mathbf{U}}^\top z &= \widehat{\mathbf{U}}^\top\mathbf{X}b\\
& = 0
\end{align*}
\item La collection des éléments de $\mathbb{R}^n$ orthogonaux à $\mathcal{S}(\mathbf{X})$ est appelée \emph{complément orthogonal} de $\mathcal{S}(\mathbf{X})$,
\begin{align*}
\mathcal{S}^\perp(\mathbf{X}) = \left\{z \in \mathbb{R}^n: z^\top\mathbf{X}=0\right\}
\end{align*}
\item Tout élément de $\mathcal{S}^\perp(\mathbf{X})$ est orthogonal à chaque élément de $\mathcal{S}(\mathbf{X})$.
\item La solution au problème des moindres carrés est donnée par,
\begin{align*}
\widehat{\mathbf{Y}} &= \mathbf{X}\widehat{\beta}\\
&=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}\\
&=\mathbf{P}_{\mathbf{X}}\mathbf{Y}
\end{align*}
où 
\begin{align*} \mathbf{P}_{\mathbf{X}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top
\end{align*}
est appelée \emph{matrice de projection orthogonale}. 
\item Pour tout vecteur $\mathbf{Y}\in\mathbb{R}^n$,
\begin{align*}
\mathbf{P}_{\mathbf{X}}\mathbf{Y} \in \mathcal{S}(\mathbf{X})
\end{align*}
\item En outre, le vecteur des résidus est dans  $\mathcal{S}^\perp(\mathbf{X})$,
\begin{align}
\mathbf{Y} - \mathbf{P}_{\mathbf{X}}\mathbf{Y} \in \mathcal{S}^\perp(\mathbf{X})
\label{eq15}
\end{align}
\item Pour montrer \eqref{eq15}, notons d'abord, qu'étant donné que les colonnes de $\mathbf{X}$ sont dans $\mathcal{S}(\mathbf{X})$,
\begin{align*}
\mathbf{P}_\mathbf{X}\mathbf{X} &= \mathbf{X}(\mathbf{X}\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}\\
&=\mathbf{X}
\end{align*}
et comme $\mathbf{P}_\mathbf{X}$ est une matrice symétrique,
\begin{align*}
\mathbf{X}^\top\mathbf{P}_\mathbf{X} = \mathbf{X}^\top
\end{align*}
Maintenant,
\begin{align*}
\mathbf{X}^\top(\mathbf{Y} - \mathbf{P}_\mathbf{X}\mathbf{Y}) &= \mathbf{X}^\top\mathbf{Y}-\mathbf{X}^\top\mathbf{P}_\mathbf{X}\mathbf{Y}\\
& = \mathbf{X}^\top\mathbf{Y}-\mathbf{X}^\top\mathbf{Y}\\
&=0 
\end{align*}
\item Ainsi, par définition, les résidus $\mathbf{Y} - \mathbf{P}_\mathbf{X}\mathbf{Y}\in\mathcal{S}^\perp(\mathbf{X})$. Les résidus peuvent s'écrire,
\begin{align*}
\widehat{\mathbf{U}} &= \mathbf{Y}-\mathbf{P}_\mathbf{X}\mathbf{Y} \\
&= (\Id_n - \mathbf{P}_\mathbf{X})\mathbf{Y}\\
& = \mathbf{M}_\mathbf{X}\mathbf{Y}
\end{align*}
où,
\begin{align*}
\mathbf{M}_\mathbf{X} &= \Id_n - \mathbf{P}_\mathbf{X}\\
& = \Id_n - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top
\end{align*}
est une matrice de projection dans $\mathcal{S}^\perp(\mathbf{X})$.\\
\item Les matrices $\mathbf{P}_\mathbf{X}$ et $\mathbf{M}_\mathbf{X}$ présentent les propriétés suivantes.
\begin{enumerate}
\item $\mathbf{P}_\mathbf{X}+\mathbf{M}_\mathbf{X} = \Id_n$. 
 Ceci implique, que pour tout $\mathbf{Y}\in \mathbb{R}^n$,
\begin{align*}
\mathbf{Y} = \mathbf{P}_\mathbf{X}\mathbf{Y}+\mathbf{M}_\mathbf{X}\mathbf{Y}
\end{align*}
\item $\mathbf{P}_\mathbf{X}$ et $\mathbf{M}_\mathbf{X}$ sont symétriques,
\begin{align*}
\mathbf{P}_\mathbf{X}^\top = \mathbf{P}_\mathbf{X}, \ \ \ \mathbf{M}_\mathbf{X}^\top= \mathbf{M}_\mathbf{X}
\end{align*}
\item $\mathbf{P}_\mathbf{X}$ et $\mathbf{M}_\mathbf{X}$ sont idempotentes,
\begin{align*}
\mathbf{P}_\mathbf{X}\mathbf{P}_\mathbf{X} = \mathbf{P}_\mathbf{X}, \ \ \ \mathbf{M}_\mathbf{X} \mathbf{M}_\mathbf{X}= \mathbf{M}_\mathbf{X}
\end{align*}
 En effet,
\begin{align*}
\mathbf{P}_\mathbf{X}\mathbf{P}_\mathbf{X} &= \left(\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\right)\left(\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\right)\\ 
&= \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \\
&= \mathbf{P}_\mathbf{X}
\end{align*}
de même,
\begin{align*}
\mathbf{M}_\mathbf{X}\mathbf{M}_\mathbf{X} &= (\Id_n - \mathbf{P}_\mathbf{X})(\Id_n - \mathbf{P}_\mathbf{X})\\
& = \Id_n - 2\mathbf{P}_\mathbf{X} + \mathbf{P}_\mathbf{X}\mathbf{P}_\mathbf{X}\\
&=\Id_n - \mathbf{P}_\mathbf{X}\\
&=\mathbf{M}_\mathbf{X}
\end{align*}

\item $\mathbf{P}_\mathbf{X}$ et $\mathbf{M}_\mathbf{X}$ sont orthogonales,
\begin{align*}
\mathbf{P}_\mathbf{X}\mathbf{M}_\mathbf{X} &= \mathbf{P}_\mathbf{X}(\Id_n -  \mathbf{P}_\mathbf{X})\\
& =  \mathbf{P}_\mathbf{X} -  \mathbf{P}_\mathbf{X} \mathbf{P}_\mathbf{X}\\
&= \mathbf{P}_\mathbf{X}- \mathbf{P}_\mathbf{X}\\
&=0
\end{align*}
Cette propriété implique que $\mathbf{M}_\mathbf{X}\mathbf{X} = 0$. En effet,
\begin{align*}
\mathbf{M}_\mathbf{X}\mathbf{X} &= (\Id_n - \mathbf{P}_\mathbf{X})\mathbf{X}\\ 
&= \mathbf{X}-\mathbf{P}_\mathbf{X}\mathbf{X}\\
& = \mathbf{X} - \mathbf{X}\\
& = 0
\end{align*}
\end{enumerate}
\item Dans la discussion ci-dessus, aucune des hypothèses quant au
  modèle de régression n'ont été utilisées. 
\item \'Etant donné des données, $\mathbf{Y}$ et $\mathbf{X}$, nous
  pouvons toujours calculer l'estimateur des moindres carrés,
  indépendamment du processus générateur des données derrière les
  données. 
\item Néanmoins, nous avons besoin d'un modèle(i.e., d'hypothèses) pour pouvoir discuter des propriétés d'un estimateur(e.g., le fait qu'il soit ou non sans biais, etc).
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Propriétés de $\widehat{\sigma}^2$}
\begin{itemize}
\item $\sigma^2$ est estimé par,
\begin{align*}
\widehat{\sigma}^2
&= n^{-1}\sumobs \widehat{U}_i^2\\
& =  n^{-1}\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}
\end{align*}
\item Mais sous \ref{hy1} - \ref{hy4} $\widehat{\sigma}^2$ est  biaisé. 
\item En effet,
\begin{align*}
\widehat{\mathbf{U}} &= \mathbf{M}_\mathbf{X}\mathbf{Y}\\
& = \mathbf{M}_\mathbf{X}(\mathbf{X}\beta + \mathbf{U})\\
& = \mathbf{M}_\mathbf{X}\mathbf{U}
\end{align*}
où la dernière égalité résulte de ce que $\mathbf{M}_\mathbf{X}\mathbf{X} = 0$. 
\item En outre,
\begin{align*}
n\widehat{\sigma}^2 &= \widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}\\
& = \mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{M}_\mathbf{X}\mathbf{U} \\
&= \mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}
\end{align*}
\item \'Etant donné que $\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}$ est un scalaire,
\begin{align*}
\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U} = \Tr\left(\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}\right)
\end{align*}
où $\Tr(A)$ désigne la trace de la matrice A.
\item  Nous avons,
\begin{align*}
\Er\left(\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}|\mathbf{X}\right) =&  \Er\left(\Tr(\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U})|\mathbf{X}\right)\\
=&\Er\left(\Tr(\mathbf{M}_\mathbf{X}\mathbf{U}\mathbf{U}^\top)|\mathbf{X}\right)(\textrm{car \ } \Tr(ABC) = \Tr(BCA))\\
=&\Tr\left(\mathbf{M}_\mathbf{X}\Er\left(\mathbf{U}\mathbf{U}^\top)|\mathbf{X}\right)\right)\\
&(\textrm{car l'opérateur trace et l'espérance sont linéaires })\\
=&\sigma^2\Tr(\mathbf{M}_\mathbf{X})
\end{align*}
\item La dernière égalité résulte de ce que par l'hypothèse \ref{hy3}, $\Er(\mathbf{U}^\top\mathbf{U}) = \sigma^2\Id_n$. 
\item Maintenant,
\begin{align*}
\Tr(\mathbf{M}_\mathbf{X}) = &\Tr\left(\Id_n - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\right)\\
=&\Tr(\Id_n) - \Tr\left(
\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\right)\\
=&\Tr(\Id_n) - \Tr\left((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}\right)\\
=&\Tr(\Id_n) - \Tr(\Id_K)\\
=&n-K
\end{align*}
\item Il s'en suit que,
\begin{align}
\Er(\widehat{\sigma}^2) = \frac{n-K}{n}\sigma^2
\label{eq16}
\end{align}
\item L'estimateur $\widehat{\sigma}^2$ est biaisé, mais le résultat précédent suggère qu'il est aisé de le modifier afin d'obtenir un estimateur sans biais. 
\item Pour cela, définissons,
\begin{align*}
s^2 &= \widehat{\sigma}^2\frac{n}{n-K}\\
&=(n-K)^{-1}\sumobs \widehat{U}_i^2
\end{align*}
\item Et il résulte de \eqref{eq16} que, 
\begin{align*}
\Er(s^2) = \sigma^2
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Régression partitionnée}
\begin{itemize}
\item Considérons,
\begin{align*}
\mathbf{X} = \left(\mathbf{X}_1 \  \mathbf{X}_2\right)
\end{align*}
et écrivons le modèle comme suit,
\begin{align*}
\mathbf{Y} = \mathbf{X}_1\beta_1 + \mathbf{X}_2\beta_2 + \mathbf{U}
\end{align*}
où $\mathbf{X}_1$ est une matrice $(n\times K_1)$, $\mathbf{X}_2$ est une matrice $(n\times K_2)$, $K_1+K_2 = K$, et,
\begin{align*}
\beta = \left(
\begin{array}{c}
\beta_1\\
\beta_2
\end{array}
\right)
\end{align*}
$\beta_1$ et $\beta_2$ étant des vecteurs de paramètres, $(K_1\times 1)$ et $(K_2\times 1)$. 
\item Concentrons nous sur  $\mathbf{X}_1$ et $\beta_1$.
 \item Soit l'estimateur des moindres carrés de $\beta$,
\begin{align*}
\widehat{\beta} = \left(
\begin{array}{c}
\widehat{\beta}_1\\
\widehat{\beta}_2
\end{array}
\right)
\end{align*}
\item Nous pouvons écrire la version suivante des équations normales,
\begin{align*}
\left(\mathbf{X}^\top\mathbf{X}\right)\widehat{\beta}=\mathbf{X}^\top\mathbf{Y}
\end{align*}
comme suit,
\begin{align*}
\left(
\begin{array}{cc}
\mathbf{X}_1^\top\mathbf{X}_1&\mathbf{X}_1^\top\mathbf{X}_2\\
\mathbf{X}_2^\top\mathbf{X}_1&\mathbf{X}_2^\top\mathbf{X}_2
\end{array}
\right)
\left(
\begin{array}{c}
\widehat{\beta}_1\\
\widehat{\beta}_2
\end{array}
\right)
=
\left(
\begin{array}{c}
\mathbf{X}_1^\top\mathbf{Y}\\
\mathbf{X}_2^\top\mathbf{Y}
\end{array}
\right)
\end{align*}
\item On peut  obtenir des expressions pour $\widehat{\beta}_1$ et $\widehat{\beta}_2$ par inversion de la matrice partitionnée à gauche de l'équation ci-dessus. 
\item Alternativement, définissons $\mathbf{M}_2$ comme la matrice de projection sur l'espace orthogonal à l'espace $\mathcal{S}(\mathbf{X}_2)$,
\begin{align*}
\mathbf{M}_2 = \Id_n - \mathbf{X}_2(\mathbf{X}_2^\top\mathbf{X}_2)^{-1}\mathbf{X}_2^\top
\end{align*}
alors,
\begin{align}
\widehat{\beta}_1 = (\mathbf{X}_1^\top\mathbf{M}_2 \mathbf{X}_1)^{-1}\mathbf{X}_1^\top\mathbf{M}_2\mathbf{Y}
\label{eq17}
\end{align}
\item Pour montrer cela, commençons par écrire,
\begin{align}
\mathbf{Y} = \mathbf{X}_1\widehat{\beta}_1+\mathbf{X}_2\widehat{\beta}_2 + \widehat{\mathbf{U}}
\label{eq18}
\end{align} 
\item Notons que par construction,
\begin{align*}
\mathbf{M}_2\widehat{\mathbf{U}} &= \widehat{\mathbf{U}}(\widehat{\mathbf{U}} \ \textrm{est orthogonal à} \ \mathbf{X}_2)\\
\mathbf{M}_2\mathbf{X}_2 &= 0\\
\mathbf{X}_1^\top\widehat{\mathbf{U}} &=0\\
\mathbf{X}_2^\top\widehat{\mathbf{U}} &=0
\end{align*}
\item Substituons l'équation \eqref{eq18} dans la partie droite de l'équation \eqref{eq17},
\begin{align*}
\left(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1\right)^{-1}
\mathbf{X}_1^\top\mathbf{M}_2\mathbf{Y}&=\left(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1\right)^{-1}
\mathbf{X}_1^\top\mathbf{M}_2\left(\mathbf{X}_1\widehat{\beta}_1+\mathbf{X}_2\widehat{\beta}_2 + \widehat{\mathbf{U}}\right)\\
&=\left(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1\right)^{-1}
\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1\widehat{\beta}_1\\
 &\ \ \ +
\left(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1\right)^{-1}
\mathbf{X}_1^\top\widehat{\mathbf{U}} \ \ (\textrm{car \ }\mathbf{M}_2\mathbf{X}_2 = 0 \textrm{ \ et \ } 
\mathbf{M}_2\widehat{\mathbf{U}} = \widehat{\mathbf{U}})\\
&=\widehat{\beta}_1
\end{align*}
\item \'Etant donné que $\mathbf{M}_2$ est symétrique et idempotente, on peut écrire,
\begin{align*}
\widehat{\beta}_1 &= \left((\mathbf{M}_2\mathbf{X}_1)^\top(\mathbf{M}_2\mathbf{X}_1)\right)^{-1}
(\mathbf{M}_2\mathbf{X}_1)^\top(\mathbf{M}_2\mathbf{Y})\\
&=\left(\tilde{\mathbf{X}}_1^\top\tilde{\mathbf{X}}_1\right)^{-1}\tilde{\mathbf{X}}_1\tilde{\mathbf{Y}}
\end{align*}
où,
\begin{align*}
\tilde{\mathbf{X}}_1&=\mathbf{M}_2\mathbf{X}_1\\
&= \mathbf{X}_1 - \mathbf{X}_2(\mathbf{X}_2^\top\mathbf{X}_2)^{-1}\mathbf{X}_2^\top\mathbf{X}_1
\end{align*}
à savoir les résidus de la régression de $\mathbf{X}_1$ sur $\mathbf{X}_2$. Et où,
\begin{align*}
\tilde{\mathbf{Y}}&=\mathbf{M}_2\mathbf{Y}\\
&= \mathbf{Y} - \mathbf{X}_2(\mathbf{X}_2^\top\mathbf{X}_2)^{-1}\mathbf{X}_2^\top\mathbf{Y}
\end{align*}
à savoir les résidus de la régression de $\mathbf{Y}$ sur $\mathbf{X}_2$.
\item Ainsi, pour obtenir les coefficients de $K_1$ premiers régresseurs, plutôt que de réaliser la régression avec les $K_1+K_2 =K$ régresseurs,
\begin{itemize}
\item on peut régresser $\mathbf{Y}$ sur $\mathbf{X}_2$ pour obtenir les résidus $\tilde{\mathbf{Y}}$, 
\item régresser $\mathbf{X}_1$ sur $\mathbf{X}_2$ pour obtenir les résidus $\tilde{\mathbf{X}}_1$, 
\item et alors régresser $\tilde{\mathbf{Y}}$ sur $\tilde{\mathbf{X}}_1$ pour obtenir $\widehat{\beta}_1$. 
\end{itemize}
\item Autrement dit, $\widehat{\beta}_1$ décrit l'effet de $\mathbf{X}_1$ une fois que ceux de $\mathbf{X}_2$ ont été contrôlés.
\item De manière similaire que pour $\widehat{\beta}_1$, nous avons pour $\widehat{\beta}_2$,
\begin{align*}
\widehat{\beta}_2 = (\mathbf{X}_2^\top\mathbf{M}_1\mathbf{X}_2)^{-1} \mathbf{X}_2^\top\mathbf{M}_1\mathbf{Y}
\end{align*}
où,
\begin{align*}
\mathbf{M}_1 = \Id_n - \mathbf{X}_1(\mathbf{X}_1^\top\mathbf{X}_1)^{-1}\mathbf{X}_1^\top
\end{align*}
\item Prenons comme exemple le modèle suivant,
\begin{align*}
Y_i = \beta_1 + \beta_2 X_i + U_i, \ \ \ i = 1,2,...,n
\end{align*}
Soit $\mathbf{1}_n$ le vecteur unitaire $(n\times 1)$, i.e.,
\begin{align*}
\mathbf{1}_n =
\left(
\begin{array}{c}
1\\
1\\
.\\
.\\
.\\
1
\end{array}
\right)
\end{align*}
\item La matrice des régresseurs est alors,
\begin{align*}
 \left(\uvec_n \ \ X\right) = 
\left(
\begin{array}{cc}
1&X_1\\
1&X_2\\
.&.\\
.&.\\
.&.\\
1&X_n
\end{array}
\right)
\end{align*}
\item Considérons,
\begin{align*}
\mathbf{M}_1 = \Id_n - \uvec_n(\uvec_n^\top\uvec_n)^{-1}\uvec_n^\top
\end{align*}
et,
\begin{align*}
\widehat{\beta}_2 = \frac{X^\top\mathbf{M}_1\mathbf{Y}}{X^\top\mathbf{M}_1X}
\end{align*}
\item Nous avons, $\uvec_n^\top\uvec_n = n$, par conséquent,
\begin{align*}
\mathbf{M}_1 = \Id_n - \frac{\uvec_n\uvec_n^\top}{n}
\end{align*}
et,
\begin{align*}
\mathbf{M}_1X &= X - \uvec_n\frac{\uvec_n^\top X}{n}\\
& = X - \overline{X}\uvec_n\\
& = 
\left(
\begin{array}{c}
X_1 - \overline{X}\\
X_2 - \overline{X}\\
.\\
.\\
.\\
X_n - \overline{X}
\end{array}
\right)
\end{align*}
où,
\begin{align*}
\overline{X} &= \frac{\uvec_n^\top X}{n}\\
&=n^{-1}\sumobs X_i
\end{align*}
\item Ainsi la matrice $\mathbf{M}_1$ transforme le vecteur $X$ en un vecteur dont les éléments sont les écarts des observations $X_i$ à leur moyenne. 
\item Et nous pouvons écrire,
\begin{align*}
\widehat{\beta}_2 &= \frac{\sumobs(X_i - \overline{X})Y_i}{\sumobs(X_i - \overline{X}) ^2}\\
&= \frac{\sumobs(X_i - \overline{X})(Y_i - \overline{Y})}{\sumobs(X_i - \overline{X}) ^2}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{Qualité de l'ajustement et coefficient de détermination ou $R^2$}
\begin{itemize}
\item \'Ecrivons,
\begin{align*}
\mathbf{Y} &= \mathbf{P}_\mathbf{X}\mathbf{Y}+ \mathbf{M}_\mathbf{X}\mathbf{Y}\\
&=\widehat{\mathbf{Y}}+\widehat{\mathbf{U}}
\end{align*}
où par construction,
\begin{align*}
\widehat{\mathbf{Y}}^\top\widehat{\mathbf{U}} &= (\mathbf{P}_\mathbf{X}\mathbf{Y})^\top (\mathbf{M}_\mathbf{X}\mathbf{Y})\\ 
&= \mathbf{Y}^\top  \mathbf{P}_\mathbf{X} \mathbf{M}_\mathbf{X}\mathbf{Y}\\
& = 0
\end{align*}
\item Supposons que le modèle contienne une constante, par exemple la première colonne de la matrice des régresseurs $\mathbf{X}$ est le vecteur unitaire $\uvec_n$.
\item  La \emph{variation totale} dans $\mathbf{Y}$ est,
\begin{align*}
\sumobs(Y_i - \overline{Y})^2 &= \mathbf{Y}^\top\mathbf{M}_1\mathbf{Y}\\
&=(\widehat{\mathbf{Y}}  + \widehat{\mathbf{U}} )^\top\mathbf{M}_1(\widehat{\mathbf{Y}}  + \widehat{\mathbf{U}} )\\
&=\widehat{\mathbf{Y}}^\top\mathbf{M}_1\widehat{\mathbf{Y}} + \widehat{\mathbf{U}}^\top\mathbf{M}_1\widehat{\mathbf{U}} + 2\widehat{\mathbf{Y}}^\top\mathbf{M}_1\widehat{\mathbf{U}}
\end{align*}
où $\overline{Y} = n^{-1}\sumobs Y_i$.
\item  Comme le modèle contient une constante,
\begin{align*}
\uvec_n^\top\widehat{\mathbf{U}} = 0
\end{align*}
et,
\begin{align*}
\mathbf{M}_1\widehat{\mathbf{U}} =\widehat{\mathbf{U}}  
\end{align*}
\item Cependant, $\widehat{\mathbf{Y}}^\top \widehat{\mathbf{U}} =0$, et par conséquent,
\begin{align*}
\mathbf{Y}^\top\mathbf{M}_1\mathbf{Y} = \widehat{\mathbf{Y}}^\top\mathbf{M}_1\widehat{\mathbf{Y}} +
\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}} 
\end{align*}
ou,
\begin{align*}
\sumobs(Y_i-\overline{Y})^2 = \sumobs(\widehat{Y}_i - \overline{\widehat{Y}})^2 + \sumobs \widehat{U}_i^2
\end{align*}
où $\overline{\widehat{Y}} = n^{-1}\sumobs \widehat{Y}_i$. 
\item Notons que,
\begin{align*}
\overline{Y} &= \frac{\uvec_n^\top\mathbf{Y}}{n} \\
&= \frac{\uvec_n^\top\widehat{\mathbf{Y}}}{n}
+ \frac{\uvec_n^\top\widehat{\mathbf{U}}}{n}\\
& = \frac{\uvec_n^\top\widehat{\mathbf{Y}}}{n} \\
&= \overline{\widehat{Y}}
\end{align*}
\item Ainsi, la moyenne des $Y_i$ et celle de leurs valeurs ajustées $\widehat{Y}_i$ étant égales, nous pouvons écrire,
\begin{align*}
\sumobs(Y_i-\overline{Y})^2 = \sumobs(\widehat{Y}_i - \overline{Y})^2 + \sumobs \widehat{U}_i^2
\end{align*}
ou,
\begin{align*}
SCT = SCE + SCR
\end{align*}
où,
$SCT := \sumobs(Y_i-\overline{Y})^2$ est la \emph{somme des carrés totale},  $SCE :=  \sumobs(\widehat{Y}_i - \overline{Y})^2$  est la \emph{somme des carrés expliqués}, et $SCR:= \sumobs \widehat{U}_i^2$ est la \emph{somme des carrés des résidus}.
\item Le rapport de la $SCE$ à la $SCT$ est appelé coefficient de détermination(On l'appelle/prononce généralement "R deux") ou $R^2$,
\begin{align*}
R^2 &= \frac{SCE}{SCT}\\
& = \frac{ \sumobs(\widehat{Y}_i - \overline{Y})^2}{\sumobs(Y_i-\overline{Y})^2}\\
&=1 - \frac{\sumobs \widehat{U}_i^2}{\sumobs(Y_i-\overline{Y})^2}\\
& = 1 - \frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}}{\mathbf{Y}^\top\mathbf{M}_1\mathbf{Y}}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{Propriétés du $R^2$}
\begin{enumerate}
\item Le $R^2$ est borné entre 0 et 1 ainsi que cela est indiqué par sa décomposition. Remarquez néanmoins que ceci n'est plus vrai dans un modèle sans constante, et dans ce cas il est indiqué de ne pas utiliser la définition précédente du $R^2$. Remarquez aussi que si $R^2 =  1$ alors $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}} = 0$, ce qui sera vrai seulement si $\mathbf{Y}\in \mathcal{S}(X)$, i.e., $\mathbf{Y}$ est \emph{exactement} une combinaison linéaire des colonnes de $\mathbf{X}$.
\item Le $R^2$ augmente avec le nombre de régresseurs. 
\item Le $R^2$ indique la part de la variation de $\mathbf{Y}$ dans l'échantillon qui est expliquée par $\mathbf{X}$. Cependant notre objectif n'est pas d'expliquer des variations dans l'échantillon mais celle de la population (dont est tiré l'échantillon). Il en résulte qu'un $R^2$ élevé n'est pas nécessairement un indicateur d'un bon modèle de régression et un $R^2$ faible n'est pas non plus un argument en défaveur du modèle considéré.
\item Il est toujours possible de trouver une matrice de régresseurs $\mathbf{X}$ pour laquelle $R^2 = 1$, il suffit de prendre $n$ vecteurs linéairement indépendants. En effet, un tel ensemble de vecteurs génère tout l'espace $\mathbb{R}^n$ de sorte que tout vecteur $\mathbf{Y}\in\mathbb{R}^n$ peut s'écrire comme une combinaison linéaire exacte des colonnes de $\mathbf{X}$.
\end{enumerate}
\end{frame}


\begin{frame}[allowframebreaks]\frametitle{$R^2$ ajusté}
\begin{itemize}
\item \'Etant donné que le $R^2$ augmente avec le nombre de régresseurs, une mesure alternative pour juger de la qualité de la régression est le $R^2$ \emph{ajusté},
\begin{align*}
\overline{R}^2 &= 1 - \frac{n-1}{n-K}(1 - R^2)\\
& = 1 - \frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}/(n-K)}{\mathbf{Y}^\top\mathbf{M}_1\mathbf{Y}/(n-1)}
\end{align*}
\item Le $R^2$ ajusté diminue la qualité de ajustement lorsque le nombre de régresseurs augmente relativement au nombre d'observations de sorte que $\overline{R}^2 $ peut diminuer avec le nombre de régresseurs. 
\item Cependant il n'y a pas vraiment d'argument fort pour utiliser une telle mesure de l'ajustement.
\end{itemize}
\end{frame}

\section{Intervalles de confiance}
\frame{\sectionpage}

\begin{frame}[allowframebreaks]\frametitle{Introduction}
\begin{itemize}
\item On considère le modèle de régression normal défini par les hypothèse \ref{hy1}-\ref{hy5}.
\item L'\emph{estimateur ponctuel} $\widehat{\beta}$, n'est pas très informatif dans la mesure où $\Pr(\widehat{\beta}=\beta)=0$. 
\item C'est pour cela qu'on s'intéresse ici à des intervalles(régions) aléatoires qui présentent la propriété d'inclure la vraie valeur du paramètre avec une certaine probabilité spécifiée $(1-\alpha)$, où $\alpha$ est un nombre "petit" appelé \emph{niveau de confiance}(e.g., $0.01$, $0.05$, $0.10$).
\item Un intervalle de confiance avec une probabilité  $(1-\alpha)$ de couvrir $\beta$ est noté $\CI_{1-\alpha}$.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{Cas scalaire}

On cherche à construire un intervalle de confiance pour le paramètre $\beta_1$ dans la régression partitionnée,
\begin{align*}
\mathbf{Y} = \beta_1\mathbf{X}_1+\mathbf{X}_2\beta_2 + \mathbf{U}
\end{align*}
où $\mathbf{X}_1$ est un vecteur $(n\times 1)$ contenant les valeurs observées du premier régresseur.

\medskip

L'estimateur des moindres carrés de $\beta_1$ est,
\begin{align*}
\widehat{\beta}_1 = \frac{\mathbf{X}_1^\top\mathbf{M}_2\mathbf{Y}}{\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1}
\end{align*}
où $\mathbf{M}_2 = \Id_n - \mathbf{X}_2(\mathbf{X}_2^\top\mathbf{X}_2)^{-1}\mathbf{X}_2$. 

\medskip

 Une méthode pour construire un intervalle de confiance consiste à considérer des intervalles symétriques autour de l'estimateur ponctuel,
 
\begin{align}
\CI_{1-\alpha} = \left[\widehat{\beta}_1- c,  \widehat{\beta}_1+c\right]
\label{eq19}
\end{align}

Comme $\widehat{\beta}_1$ est une fonction de l'échantillon aléatoire, l'intervalle de confiance donné dans \eqref{eq19} l'est aussi. 

\medskip
Le problème maintenant est de choisir $c$ tel que,
\begin{align*}
\Pr(\beta_1 \in \CI_{1-\alpha}| \mathbf{X}) = 1-\alpha
\end{align*}
où $\mathbf{X} = (\mathbf{X}_1 \ \ \mathbf{X}_2)$. 

\medskip

Pour choisir $c$, nous avons besoin de connaître la distribution de $\widehat{\beta}_1|\mathbf{X}$. 

\framebreak

Sous les hypothèses \ref{hy1}-\ref{hy5},
\begin{align*}
\widehat{\beta}_1|\mathbf{X} \sim \mathcal{N}\left(\beta_1, \sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)\right)
\end{align*}
et par conséquent,
\begin{align}
\frac{\widehat{\beta}_1-\beta_1 }{\sqrt{\sigma^2
(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} | \mathbf{X} \sim \mathcal{N}(0, 1)
\label{eq20}
\end{align}

Pour montrer ce résultat, notons que $\widehat{\beta}_1$ est un estimateur linéaire, et écrivons $\widehat{\beta}_1 = \beta_1 + (\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}) / (\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)$.

Soit $z_\tau$ le quantile $\tau$ de la distribution normale standard; autrement dit, si $Z\sim \mathcal{N}(0,1)$,
\begin{align*}
\Pr(Z \leq z_\tau) = \tau
\end{align*}

Notons qu'étant donné que la distribution normale standard est symétrique autour de zéro, nous avons,
\begin{align*}
z_\alpha = -z_{(1-\alpha)}
\end{align*}
et par conséquent,
\begin{align*}
\Pr(-z_{1 - \alpha/2}\leq Z\leq z_{1-\alpha/2}) = 1-\alpha
\end{align*}
Par exemple, pour $\alpha=0.05$, $z_{1 - 0.05/2} = z_{0.975} = 1.96$, et $z_{0.025} = -1.96$.
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{$\sigma^2$ est connu}
\begin{itemize}
\item Supposons pour le moment que $\sigma^2$ soit connu et que ce faisant nous puissions calculer la variance de $\widehat{\beta}_1$(et non pas un estimateur). 
\item Posons,
\begin{align*}
c = z_{1-\alpha/2}\sqrt{\Var\left(\widehat{\beta}_1| \mathbf{X} \right)}=z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}
\end{align*}
\item Montrons maintenant que,
\begin{align*}
\Pr\left(
\beta_1 \in \left[\widehat{\beta}_1 -z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)} , \widehat{\beta}_1+
z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)} \right] | \mathbf{X}\right) = 1-\alpha
\end{align*}
\item En effet,
\begin{align}
\Pr\left(
\widehat{\beta}_1 -z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}
\leq \beta_1 \leq
\widehat{\beta}_1+
z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}  | \mathbf{X}
\right) \nonumber\\
=\Pr\left(
-z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}
\leq \beta_1 - \widehat{\beta}_1 \leq
z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}  | \mathbf{X}
\right)\nonumber\\
=\Pr\left(
-z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}
\leq  \widehat{\beta}_1-\beta_1 \leq
z_{1-\alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}  | \mathbf{X}
\right)\nonumber\\
=
\Pr\left(
-z_{1-\alpha/2}
\leq  \frac{\widehat{\beta}_1-\beta_1}{\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} \leq
z_{1-\alpha/2}  | \mathbf{X}
\right)
\label{eq21}
\end{align}
\item Le résultat découle de \eqref{eq20}, \eqref{eq21}, et de la définition de $z_{1-\alpha/2}$.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{$\sigma^2$ est inconnu}
\begin{itemize}
\item  On peut ici suivre une approche similaire à la précédente mais en remplaçant dans un premier temps $\sigma^2$ par son estimateur,
\begin{align*}
s^2 = \widehat{\mathbf{U}}^\top \widehat{\mathbf{U}}/(n-K)
\end{align*}
\item Cependant $(\widehat{\beta}_1-\beta_1)/
\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}$ n'est pas normalement distribué car c'est une fonction non-linéaire des termes aléatoires $\widehat{\beta}_1$ et $s^2$. 
\item Il s'en suit que nous ne pouvons pas utiliser les quantiles de la distribution normale pour la construction des intervalles de confiance.
\item En fait, il s'avère que,
\begin{align}
\frac{(\widehat{\beta}_1-\beta_1)}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}
| \mathbf{X}
\sim t_{n-K}
\label{eq22}
\end{align}
\item Rappelons que la distribution $t_{n-K}$ est définie comme suit,
\begin{align*}
Z/\sqrt{V/(n-K)}
\end{align*}
où $Z\sim \mathcal{N}(0,1)$, $V\sim \chi^2_{n-K}$, et $Z$ et $V$ sont indépendantes.
\item  \'Ecrivons,
\begin{align}
\frac{\widehat{\beta}_1-\beta_1}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}
&= \left(
\frac{\widehat{\beta}_1-\beta_1}{\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}
\right)/
\frac{s^2}{\sigma^2}\nonumber \\
&= 
 \left(
\frac{\widehat{\beta}_1-\beta_1}{\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}
\right)/
\sqrt{\frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}}{\sigma^2}/(n-K)}
\label{eq32}
\end{align}
\item Nous savons déjà que, dans l'expression précédente, $(\widehat{\beta}_1-\beta_1)/\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)} |\mathbf{X} \sim \mathcal{N}(0,1)$. 
\item Nous allons montrer maintenant que conditionnellement à $\mathbf{X}$,
\begin{align}
\frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}}{\sigma^2} | \mathbf{X}\sim\chi^2_{n-K}
\label{eq23}
\end{align}
\item Pour cela nous avons besoin du résultat suivant,
\begin{lemme}
\item Supposons que le vecteur $(n\times 1)$ $U\sim\mathcal{N}(0, \Id_n)$. Soit $A$ une matrice $(n\times n)$ symétrique et idempotente avec $\Rang(A) = r \leq n$. 
\item Alors $U^\top A U \sim \chi^2_r$.
\label{le1}
\end{lemme}
\begin{proof}
(voir notes de cours)
\end{proof}
\item A présent pour montrer \eqref{eq23}, écrivons,
\begin{align}
\frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}}{\sigma^2} = \left(\frac{\mathbf{U}}{\sigma^2}\right)^\top\mathbf{M}_\mathbf{X}\left(\frac{\mathbf{U}}{\sigma^2}\right)
\label{eq24}
\end{align}
où,
\begin{align*}
\mathbf{M}_\mathbf{X}=\Id_n - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top
\end{align*}
\item Par l'hypothèse \ref{hy5},
\begin{align}
\frac{\mathbf{U}}{\sigma} | \mathbf{X}\sim\mathcal{N}(0, \Id_n)
\label{eq25}
\end{align}
\item \'Etant donné que $\mathbf{M}_\mathbf{X}$ est symétrique et idempotente, ses valeurs propres sont soit zéro ou un. 
\item Par conséquent,
\begin{align}
\Rang(\mathbf{M}_\mathbf{X}) &= \Tr(\mathbf{M}_\mathbf{X}) \nonumber\\
&=n-K
\label{eq26}
\end{align}
\item Le résultat dans \eqref{eq23} découle de \eqref{eq24},  \eqref{eq25},  \eqref{eq26}, et du lemme \ref{le1}.
\item Finalement, montrons que $\widehat{\beta}_1 - \beta_1$ et $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}$ dans \eqref{eq32} sont indépendants conditionnellement à $\mathbf{X}$. 
\item \'Ecrivons,
\begin{align*}
\widehat{\beta}_1-\beta_1 &= (\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U})/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)\\
\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}&=\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}
\end{align*}
\item Il suffit de montrer l'indépendance de $\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}$ et  $\mathbf{M}_\mathbf{X}\mathbf{U}$.
\item  Comme $\widehat{\beta}_1$ est une fonction de $\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}$, et $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}$ est une fonction de $\mathbf{M}_\mathbf{X}\mathbf{U}$, l'indépendance de $\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}$ et  $\mathbf{M}_\mathbf{X}\mathbf{U}$ implique l'indépendance de $\widehat{\beta}_1$ et $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}$. 
\item Premièrement, montrons que les termes ne sont pas corrélés,
\begin{align*}
\Cov(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}, \mathbf{M}_\mathbf{X}\mathbf{U} | \mathbf{X}) &=\Er(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}\mathbf{U}^\top \mathbf{M}_\mathbf{X}|\mathbf{X})\\
&=\mathbf{X}_1^\top\mathbf{M}_2\Er(\mathbf{U}\mathbf{U}^\top|\mathbf{X})\mathbf{M}_\mathbf{X}\\
&=\mathbf{X}_1^\top\mathbf{M}_2(\sigma^2\Id_n)\mathbf{M}_\mathbf{X}\\
&=\sigma^2\mathbf{X}_1^\top\mathbf{M}_2\mathbf{M}_\mathbf{X}\\
&=\sigma^2\mathbf{X}_1^\top\mathbf{M}_\mathbf{X}( \ \textrm{voir séction précédente})\\
&=0
\end{align*}
\item Dans la mesure où $\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}$ et $\mathbf{M}_\mathbf{X}\mathbf{U}$ sont des fonctions linéaires de $\mathbf{U}$, elles sont normalement distribuées conditionnellement à $\mathbf{X}$.
 \item \'Etand donné qu'elles ne sont pas corrélées, la normalité implique qu'elles sont indépendantes. 
\item En conséquence, $\widehat{\beta}_1 - \beta_1$, fonction de  $\mathbf{X}_1^\top\mathbf{M}_2\mathbf{U}$, et $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}$ sont aussi indépendants.
\item Nous avons montré \eqref{eq22}. 
\item En conséquence, en construisant des intervalles de confiance, si l'on remplace l'inconnue $\sigma^2$ par $s^2$, on doit remplacer $z_{1-\alpha/2}$ par les quantiles de la $t$ distribution, $t_{n-K, 1-\alpha/2}$,
\begin{align*}
\CI_{1-\alpha} = \left[\widehat{\beta}_1 - t_{n-K, 1-\alpha/2}\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}, 
\widehat{\beta}_1 + t_{n-K, 1-\alpha/2}\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}\right]
\end{align*}
\item L'expression $s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)$ qui apparaît dans l'équation ci-dessus est la variance estimé de $\widehat{\beta}_1$,
\begin{align*}
\widehat{\Var}(\widehat{\beta}_1| \mathbf{X}) = s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)
\end{align*}
\item Ainsi, on construit un intervalle de confiance de niveau $\alpha$ pour $\beta_k$, $k=1, 2,...,K$, comme suit,
\begin{align}
\CI_{1-\alpha} = \left[\widehat{\beta}_1 - t_{n-K, 1-\alpha/2}\sqrt{\widehat{\Var}(\widehat{\beta}_1| \mathbf{X})}, 
\widehat{\beta}_1 + t_{n-K, 1-\alpha/2}\sqrt{\widehat{\Var}(\widehat{\beta}_1| \mathbf{X})}\right]
\label{eq27}
\end{align}
\end{itemize}
\end{frame}
\begin{frame}[allowframebreaks]\frametitle{Cas vectoriel}
\begin{itemize}
\item Supposons que l'on s'intéresse aux vecteur des paramètres $\beta =(\beta_1,\beta_2,...,\beta_K)^\top$. \item L'équation \eqref{eq27} décrit comment construire des intervalles de confiance "individuels" pour les éléments de $\beta$. 
\item Ces intervalles concernent les distributions marginales des éléments de $\beta$, et leur simple combinaison ne produit pas un ensemble qui inclue tout le vecteur $\beta$ avec une probabilité souhaitée. 
\item Dans cette partie, nous considérons la construction de régions aléatoires qui incluent $\beta$ avec une certaine probabilité pré-spécifiée $1-\alpha$. 
\item Nous conservons la notation $\CI_{1-\alpha}$, malgré le fait que  $\CI_{1-\alpha}$ est maintenant un sous-ensemble de $\mathbb{R}^K$.
\item Ce qui suit est une approche simple et conventionnelle pour construire des régions de confiance. 
\item Nous cherchons une région de confiance $\CI_{1-\alpha} = \{b \in \mathbb{R}^K\}$ tel que $\Pr(\beta \in \CI_{1-\alpha} | \mathbf{X})=1-\alpha$. 
\item Considérons une forme quadratique par rapport à $(\widehat{\beta}-\beta)$,
\begin{align}
(\widehat{\beta}-\beta)^\top\left(\widehat{\Var}(\widehat{\beta}|\mathbf{X})\right)^{-1}(\widehat{\beta}-\beta)/K &=
(\widehat{\beta}-\beta)^\top\left(s^2(\mathbf{X}^\top\mathbf{X})^{-1}
\right)^{-1}
(\widehat{\beta}-\beta)/K \nonumber\\
&=\frac{
(\widehat{\beta}-\beta)^\top\left(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\right)^{-1}
(\widehat{\beta}-\beta)/K}
{s^2/\sigma^2}\nonumber\\
&=\frac{
(\widehat{\beta}-\beta)^\top\left(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\right)^{-1}
(\widehat{\beta}-\beta)/K}
{\left(\frac{\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}}{\sigma^2}\right)/(n-K)}
\label{eq29}
\end{align}
\item Montrons maintenant que l'expression dans \eqref{eq29} possède une distribution $F_{K,n-K}$ conditionnellement à $\mathbf{X}$.
\item La distribution $F_{K,n-K}$ est définie comme la distribution de,
\begin{align*}
\frac{V/K}{W/(n-K)}
\end{align*}
où $V \sim \chi^2_K$, et $W\sim\chi^2_{n-K}$ sont indépendantes. 
\item De la discussion dans la partie précédente nous savons que, $\widehat{\mathbf{U}}^\top\widehat{\mathbf{U}}/\sigma^2|\mathbf{X}\sim\chi^2_{n-K}$ qui est indépendant du numérateur dans \eqref{eq29}.
\item  Il résulte de cela, que nous devons montrer que 
\begin{align}
(\widehat{\beta}-\beta)^\top\left(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\right)^{-1}
(\widehat{\beta}-\beta) | \mathbf{X}\sim \chi^2_K
\label{eq30}
\end{align}
\item Pour cela nous avons besoin du lemme suivant,
\begin{lemme}
Supposons que le vecteur $(K\times 1)$, $U\sim\mathcal{N}(0, \Sigma)$, où $\Sigma$ est une matrice définie positive de variances-covariances. Alors, $U^\top\Sigma^{-1}U\sim\chi^2_K$.
\label{le2}
\end{lemme}
\begin{proof}
(voir notes de cours)
\end{proof}

\item Le résultat dans $\eqref{eq30}$ découle du lemme \ref{le2}. 
\item En conséquence,
\begin{align*}
\frac{
(\widehat{\beta}-\beta)^\top\left(s^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)^{-1}(\widehat{\beta}-\beta)
}{K}|\mathbf{X}\sim F_{K, n-K}
\end{align*}
\item Soit, $ F_{K, n-K, \tau}$ le quantile $\tau$ de la distribution $F$. La région de confiance de niveau $\alpha$ se construit comme suit,
\begin{align*}
\CI_{1-\alpha} = \left\{b \in \mathbb{R}^K: (\widehat{\beta}-b)^\top\left(s^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)^{-1}(\widehat{\beta}-b)/K \leq F_{K, n-K, 1-\alpha}\right\}
\end{align*}
\item La discussion précédente implique que,
\begin{align*}
\Pr(\beta \in \CI_{1-\alpha}|\mathbf{X}) &=\Pr\left( (\widehat{\beta}-\beta)^\top\left(s^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)^{-1}(\widehat{\beta}-\beta)/K \leq F_{K, n-K, 1-\alpha} | \mathbf{X}\right)\\
&=1-\alpha 
\end{align*}
\begin{remarque}
\item La région/intervalle de confiance $\CI_{1-\alpha}$ est une fonction de l'échantillon $\{(Y_i, X_i)\}_{i=1}^n$, et il est ce faisant aléatoire, ce qui nous permet de parler de la probabilité que $\CI_{1-\alpha}$ contienne la vraie valeur de $\beta$. 
\item D'un autre côté, la réalisation de $\CI_{1-\alpha}$ n'est pas aléatoire. Une fois que l'intervalle de confiance est calculé pour des observations données, il n'y a plus de sens à parler de la probabilité qu'il inclue $\beta$. C'est soit zéro, soit un.
\end{remarque}
\end{itemize}
\end{frame}

\section{Tests d'hypothèses}
\frame{\sectionpage}
\begin{frame}[allowframebreaks]{Test d'une hypothèse par rapport à un seul coefficient}
Nous poursuivons notre discussion sur le modèle de régression linéaire normal, i.e., le modèle défini par  les hypothèses \ref{hy1}-\ref{hy5}.
On considère,
\begin{align*}
\mathbf{Y} = \beta_1\mathbf{X}_1 + \mathbf{X}_2\beta_2 + \mathbf{U}
\end{align*}
où $\mathbf{X}_1$ est le vecteur $(n\times 1)$ d'observations du premier régresseur. 

\medskip

Supposons que la variance des erreurs $\sigma^2$ soit connue. 

\medskip

Soit $\widehat{\beta}_1$ l'estimateur des moindres carrés de $\beta_1$.  

\medskip

Cherchons à tester,
\begin{align}
H_0 \ : \  \beta_1 = \beta_{1, 0}\nonumber\\
H_1 \ : \  \beta_1 \neq \beta_{1, 0}
\label{eq34}
\end{align}

\medskip

Une règle de décision pour un test de niveau $\alpha$ peut reposer  sur l'intervalle de confiance $\CI_{1-\alpha}$. 

\medskip

Nous avons,
\begin{align*}
\CI_{1-\alpha} = \left[ \widehat{\beta}_1 - z_{1 - \alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)},  \widehat{\beta}_1 + z_{1 - \alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}\right]
\end{align*}

Considérons le test suivant,

\begin{align*}
\textrm{Rejeter} \ H_0 \  \textrm{si} \ \beta_{1,0} \notin \CI_{1-\alpha}
\end{align*}

Dans ce cas la région critique est donnée par le complément de $ \CI_{1-\alpha}$. 

\medskip

On rejette ainsi $H_0$ si,
\begin{align*}
\beta_{1,0}&\leq  \widehat{\beta}_1 - z_{1 - \alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}\\
\textrm{ou}&\\
\beta_{1,0}&\geq  \widehat{\beta}_1 + z_{1 - \alpha/2}\sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}
\end{align*}

\medskip

De manière équivalente, on rejette si,

\begin{align}
\abs{\frac{\widehat{\beta}_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}} > z_{1-\alpha/2}
\label{eq35}
\end{align}

Un tel test est appelé \emph{bilatéral}, car sous l'hypothèse alternative, la vraie valeur $\beta_1$ peut être plus petite ou plus grande que $\beta_{1,0}$.

\medskip

L'expression à gauche de l'inégalité est une statistique de test. 

\medskip
Pour calculer la probabilité de rejet de l'hypothèse nulle supposons que la vraie valeur soit donnée par $\beta_1$. \'Ecrivons, 
\begin{align}
\frac{\widehat{\beta}_1 - \beta_1}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} + 
\frac{\beta_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} 
\label{eq36}
\end{align}

Nous avons que,

\begin{align*}
\frac{\widehat{\beta}_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} | \mathbf{X}
 \sim
 \mathcal{N}\left( \frac{\beta_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}, 1 \right) 
\end{align*}
Si l'hypothèse nulle est vraie alors $\beta_1-\beta_{1,0} = 0$ et la statistique de test présente une distribution normale standard. Dans ce cas par définition de $z_{1-\alpha/2}$.
\begin{align*}
\Pr\left(\textrm{rejeter} \ H_0 | \mathbf{X}, H_0 \ \textrm{est vraie}\right) &= 
\Pr\left(\abs{\frac{\widehat{\beta}_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}} > z_{1-\alpha/2}\right)\\
&= \alpha
\end{align*}

Ainsi, le test suggéré a la taille correcte $\alpha$.

\medskip
 Si l'hypothèse nulle est fausse, la distribution de la statistique de test n'est pas centrée autour de zéro, et l'on verra des taux de rejet supérieurs à $\alpha$.
 
 \medskip
La probabilité de rejet est une fonction de la vraie valeur $\beta_1$ et dépend de la magnitude du deuxième terme dans \eqref{eq36}, $\abs{\beta_1 - \beta_{1,0}}/ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}$. 

\medskip

Supposons par exemple que,
\begin{align*}
\beta_{1,0} &= 0\\
 \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)} &=1\\
 \alpha &=0.05(\textrm{et} \ z_{1-\alpha/2} = 1.96) 
\end{align*}
Soit $Z \sim \mathcal{N}(0,1)$. Dans ce cas, la \emph{fonction puissance} du test est,
\begin{align*}
\pi(\beta_1) &= \Pr\left( \abs{\frac{\widehat{\beta}_1 - \beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}} > z_{1-\alpha/2} \right)\\
&= \Pr\left( \abs{\frac{\widehat{\beta}_1 - \beta_1+\beta_1 -\beta_{1,0}}{ \sqrt{\sigma^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}} > 1.96 | \mathbf{X} \right)\\
&=\Pr\left(\abs{Z + \beta_1} > 1.96\right)\\
&= \Pr\left( Z < -1.96 - \beta_1\right)+ \Pr\left( Z > 1.96 - \beta_1\right)
\end{align*}
Par exemple,
\begin{align*}
\pi(\beta_1) =
\left\{
\begin{array}{c}
0.52 \ \textrm{pour} \ \beta_1 = -2\\
0.17 \ \textrm{pour} \ \beta_1 = -1\\
0.05 \ \textrm{pour} \ \beta_1 = 0\\
0.17 \ \textrm{pour} \ \beta_1 = 1\\
0.52 \ \textrm{pour} \ \beta_1 = 2
\end{array}
\right.
\end{align*}
Dans ce cas la fonction puissance est minimisée en $\beta_1 = \beta_{1,0}$ où $\pi(\beta_1) = \alpha$.

\medskip

Pour le calcul des $p-values$ considérons l'exemple suivant.

\medskip

 Supposons, qu'étant donné des données la statistique de test  dans \eqref{eq35} soit égale à $1.88$. 

\medskip
 
 Pour la distribution normale standard $\Pr(Z > 1.88) = 0.03$. 
 
\medskip 

 Par conséquent la $p-value$ du test est $0.06$. On rejeterait l'hypothèse nulle pour tous les tests avec un niveau de significativité supérieur à $0.06$.
 
 \medskip
Dans le cas où $\sigma_2$ est inconnu, on peut tester \eqref{eq34} en considérant la 
$t$-statistique,
\begin{align}
T &= \frac{\widehat{\beta}_1 - \beta_{1,0}}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}}\nonumber\\
&=\frac{\widehat{\beta}_1 - \beta_{1,0}}{\sqrt{\widehat{\Var}(\widehat{\beta}_1 | \mathbf{X})}}
\label{eq37}
\end{align}
Le test est donné par la règle de décision suivante,
\begin{align*}
\textrm{Rejeter} \ H_0 \ \textrm{si} \ \abs{T} > t_{n-K, 1-\alpha/2}
\end{align*}
Dans ce cas(voir section précédente) sous $H_0$, $\Pr( \abs{T} > t_{n-K, 1-\alpha/2} | \mathbf{X}, H_0 \ \textrm{est vraie}) = \alpha$.

\medskip

On peut aussi considérer des tests \emph{unilatéraux}. 

\medskip

Dans le cas de ces tests l'hypothèse nulle et l'hypothèse alternative peuvent être spécifiées comme suit,
\begin{align*}
H_0 &:  \beta_1 \leq \beta_{1,0}\\
H_1 &:  \beta_1 > \beta_{1,0}
\end{align*}
Notons que dans ce cas, et $H_0$ et $H_1$ sont composées, et la probabilité de rejet varie non seulement selon les valeurs de $\beta_1$ spécifiées sous $H_1$ mais aussi selon $H_0$.

\medskip
 Dans ce cas cas un test valide devra satisfaire la condition,
\begin{align}
\underset{\beta_1\leq\beta_{1,0}}{\sup}\Pr(\textrm{rejeter} \ H_0 | \mathbf{X}, \beta_1) \leq \alpha
\label{eq38}
\end{align}
i.e., la probabilité maximale de rejeter $H_0$ quand elle est vraie ne doit pas dépasser $\alpha$. Soit $T$ telle que définie dans \eqref{eq37} et considérons le test suivant(règle de décision):
\begin{align*}
\textrm{Rejeter}  \ H_0 \ \textrm{quand}  \ T > t_{n-K, 1-\alpha}
\end{align*}
Sous $H_0$, nous avons,
\begin{align*}
\Pr\left(\textrm{rejeter} \ H_0 | \beta_1 \leq \beta_{1,0}  \right)&=\Pr\left(T >  t_{n-K, 1-\alpha} | \mathbf{X}, \beta_1 \leq \beta_{1,0}\right)\\
&=\Pr\left( 
\frac{\widehat{\beta}_1 - \beta_{1,0}}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} >  t_{n-K, 1-\alpha} | \mathbf{X}, \beta_1 \leq \beta_{1,0}\right)\\
&\leq
\Pr\left( 
\frac{\widehat{\beta}_1 - \beta_1}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} >  t_{n-K, 1-\alpha} | \mathbf{X}, \beta_1 \leq \beta_{1,0}\right)(\textrm{car}  \  \beta_1 \leq \beta_{1,0})\\
&=\alpha( \textrm{étant donné que} \  \frac{\widehat{\beta}_1 - \beta_1}{\sqrt{s^2/(\mathbf{X}_1^\top\mathbf{M}_2\mathbf{X}_1)}} | \mathbf{X} \sim t_{n-K}  )
\end{align*}
Ainsi, la condition sur la taille \eqref{eq38} est satisfaite. Notons qu'étant donné qu'il s'agit d'un test unilatéral, la probabilité d'erreur de type 1 est portée uniquement par la queue droite de la distribution.
\end{frame}


\begin{frame}[allowframebreaks]{Test d'une contrainte linéaire simple}
Considérons le modèle de régression linéaire normal défini par les hypothèse \ref{hy1}-\ref{hy5},
\begin{align*}
\mathbf{Y} = \mathbf{X}\beta + \mathbf{U}
\end{align*}
Supposons que l'on souhaite tester,
\begin{align*}
H_0&: c^\top\beta=r\\
H_1&: c^\top\beta \neq r
\end{align*}
Dans ce cas $c$ est un vecteur $(K\times 1)$, $r$ est un scalaire, et sous l'hypothèse nulle,
\begin{align*}
c_1\beta_1 + x_2\beta_2+...+c_K\beta_K-r=0
\end{align*}
Par exemple, en posant $c_1 = 1$, $c_2=-1$, $c_3 = ...=c_K = 0$, et $r = 0$, nous pouvons tester l'hypothèse que $\beta_1=\beta_2$.

\medskip

Pour l'estimateur des moindres carrés nous avons,
\begin{align}
\widehat{\beta}|\mathbf{X} \sim \mathcal{N}\left(\beta, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)
\label{eq39}
\end{align} 
Alors,
\begin{align}
\frac{c^\top\widehat{\beta} - c^\top\beta}{\sqrt{\sigma^2c^\top(\mathbf{X}^\top\mathbf{X})^{-1}c}}
| \mathbf{X} \sim \mathcal{N}(0, 1)
\label{eq40}
\end{align} 
Par conséquent, sous $H_0$,
\begin{align}
\frac{c^\top\widehat{\beta} - r}
{\sqrt{\sigma^2c^\top(\mathbf{X}^\top\mathbf{X})^{-1}c}}
| \mathbf{X} \sim \mathcal{N}(0, 1)
\label{eq41}
\end{align} 
Considérons la $t$ statistique,
\begin{align*}
T &= \frac{c^\top\widehat{\beta} - r}
{\sqrt{s^2c^\top(\mathbf{X}^\top\mathbf{X})^{-1}c}}\\
&=\left(
\frac{c^\top\widehat{\beta} - r}
{\sqrt{\sigma^2c^\top(\mathbf{X}^\top\mathbf{X})^{-1}c}}
\right)
/
\sqrt{\frac{\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}}{\sigma^2}/(n-K)}
\end{align*}
Sous $H_0$, le résultat dans \eqref{eq41} est vérifié. 

\medskip

En outre, conditionnellement à $\mathbf{X}$,
\begin{align}
\mathbf{U}^\top\mathbf{M}_\mathbf{X}\mathbf{U}/\sigma^2 | \mathbf{X}\sim \chi^2_{n-K} \ \textrm{indépendant de}  \ \widehat{\beta} 
\label{eq42}
\end{align}
Par conséquent sous $H_0$,
\begin{align*}
T | \mathbf{X}\sim t_{n-K}
\end{align*}
Ainsi, le niveau de significativité $\alpha$ du test bilatéral de $H_0: c^\top\beta = r$ est donné par,
\begin{align*}
\textrm{Rejeter} \ H_0 \ \textrm{si} \ \abs{T} > t_{n-K, 1-\alpha/2}
\end{align*}
En posant l'élément $j$ de $c$, $c_j=1$ et le restant des éléments de $c$ égaux à zéro on obtient le test discuté dans la sous-section précédente,
\begin{align*}
H_0&: \beta_j = r\\
H_1&:\beta_j \neq r
\end{align*}
On rejette $H_0$ si,
\begin{align*}
\abs{T} &= \abs{ \frac{\widehat{\beta}_j - r}{\sqrt{s^2\left[(\mathbf{X}^\top\mathbf{X})^{-1}\right]_{jj}}}}\\
&> t_{n-K, 1-\alpha/2}
\end{align*}
où $\left[(\mathbf{X}^\top\mathbf{X})^{-1}\right]_{jj}$ est l'élément $(j,j)$ de la matrice $(\mathbf{X}^\top\mathbf{X})^{-1}$.
\end{frame}

\begin{frame}[allowframebreaks]{Tests de contraintes linéaires multiples}

Supposons que l'on souhaite tester,
\begin{align*}
H_0: \mathbf{R}\beta = r\\
H_1: \mathbf{R}\beta \neq r
\end{align*}
où $\mathbf{R}$ est une matrice $(q \times K)$ est $r$ est un vecteur $(r\times 1)$. Par exemple, 
\begin{itemize}
\item $\mathbf{R} = \Id_K$, $r=0$. Dans ce cas on teste que $\beta_1=...=\beta_K=0$.
\item $\mathbf{R} = \left(
\begin{array}{cccccccc}
1&1&0&0&.&.&.&0\\
0&0&1&0&.&.&.&0
\end{array}
\right)$, $r = \left(\begin{array}{c}
1\\
0
\end{array}
\right)
$. Dans ce cas, $H_0: \beta_1 + \beta_2 = 1$, $\beta_3=0$.
\end{itemize}

Considérons la $F$ statistique,
\begin{align*}
F = \left(\mathbf{R}\widehat{\beta} - r\right)^\top\left(s^2\mathbf{R}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{R}^\top\right)^{-1}\left(\mathbf{R}\widehat{\beta} - r\right)/q
\end{align*}
On peut alors montrer(voir notes de cours) que sous $H_0$,
\begin{align}
F | \mathbf{X} \sim F_{q, n-K}
\label{eq43}
\end{align}
 Par conséquent, le test est donné par,
\begin{align*}
\textrm{Rejeter} \ H_0 \ \textrm{si} \ F &= \left(\mathbf{R}\widehat{\beta} - r\right)^\top\left(s^2\mathbf{R}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{R}^\top\right)^{-1}\left(\mathbf{R}\widehat{\beta} - r\right)/q\\
&> F_{q, n-K, 1-\alpha} 
\end{align*}
\end{frame}


%\begin{frame}[allowframebreaks]\frametitle{Références.}

%\bibliographystyle{plainnat}
%\bibliography{../Biblio}

%\end{frame}

%\begin{frame}[allowframebreaks]\frametitle{References.}

%\bibliographystyle{jpe}
%\bibliography{Biblio}

%\end{frame}

\end{document}